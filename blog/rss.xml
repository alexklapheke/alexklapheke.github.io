<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
	<channel>
		<title>Alexander Klapheke</title>
		<link>https://alexklapheke.github.io/</link>
		<description></description>
		<language>en-US</language>
		<lastBuildDate>2020-07-14T20:22:55-04:00</lastBuildDate>
		<item>
			<title>Automated essay scoring</title>
			<link>https://alexklapheke.github.io/blog/publish/essay.md</link>
			<pubDate>2020-07-12T18:20:44-0400</pubDate>
	<description>
&lt;!-- https://www.forbes.com/sites/petergreene/2020/05/25/no-software-still-cant-grade-student-essays/ --&gt;
&lt;!-- https://www.ets.org/research/topics/as_nlp --&gt;
&lt;div class=&quot;epigraph&quot;&gt;
&lt;p&gt;&#8217;Tis hard to say, if greater Want of Skill&lt;br /&gt;
Appear in Writing or in Judging ill&lt;/p&gt;
&lt;/div&gt;
&lt;h1 id=&quot;prolegomenon&quot;&gt;Prolegomenon&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Automated_essay_scoring&quot;&gt;Automated Essay Scoring&lt;/a&gt; has been contemplated as an application of machine learning since its earliest days. The ETS began using its proprietary &lt;a href=&quot;https://www.ets.org/erater/about&quot;&gt;e-rater&lt;/a&gt; in 1999, which, with a human cohort, now grades the SAT essay. In 2012, the Hewlitt Foundation sponsored the &lt;a href=&quot;https://www.kaggle.com/c/asap-aes&quot;&gt;Automated Student Assessment Prize&lt;/a&gt; (ASAP), offering a $100,000 reward for the best scoring system. Not long after, &lt;span class=&quot;citation&quot; data-cites=&quot;shermis2013contrasting&quot;&gt;@shermis2013contrasting&lt;/span&gt; found that automated scoring systems performed similarly to human graders, a claim met with both &lt;a href=&quot;https://www.insidehighered.com/news/2012/04/13/large-study-shows-little-difference-between-human-and-robot-essay-graders&quot;&gt;praise&lt;/a&gt; and &lt;a href=&quot;https://www.nytimes.com/2012/04/23/education/robo-readers-used-to-grade-test-essays.html&quot;&gt;skepticism&lt;/a&gt;. Les Perelman, for example, inveighed that e-rater looked for particular stylistic cues without considering their rhetorical effect:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;E-Rater, [Perelman] said, does not like short sentences.&lt;/p&gt;
&lt;p&gt;Or short paragraphs.&lt;/p&gt;
&lt;p&gt;Or sentences that begin with &#8220;or.&#8221; And sentences that start with &#8220;and.&#8221; Nor sentence fragments.&lt;/p&gt;
&lt;p&gt;However, he said, e-Rater likes connectors, like &#8220;however,&#8221; which serve as programming proxies for complex thinking. Moreover, &#8220;moreover&#8221; is good, too.&lt;/p&gt;
&lt;p&gt;Gargantuan words are indemnified because e-Rater interprets them as a sign of lexical complexity. &#8220;Whenever possible,&#8221; Mr.&#160;Perelman advises, &#8220;use a big word. &#8216;Egregious&#8217; is better than &#8216;bad.&#8217;&#8221;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And in a more thorough rejoinder &lt;span class=&quot;citation&quot; data-cites=&quot;perelman2013critique&quot;&gt;[-@perelman2013critique]&lt;/span&gt;, Perelman contests the statistical results as cherry-picked:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The clearest omission is the failure of the authors to report the fairly large percentage of machine values for the Pearson &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;r&lt;/em&gt;&lt;/span&gt; and the Quadratic Weighted Kappa that fell below the minimum standard of 0.7. [&#8230;] Any value below 0.7 will be predicting significantly less than half the population and, because this is an exponential function, small decreases in value produce large decreases in the percentage accurately predicted. [&#8230;] Yet for the Quadratic Weighted Kappa, 28 of the 81 machine scores, 35.6%, are below the minimally acceptable level of 0.7, even though the machines had the advantage in half of the essay sets of matching an inflated Resolved Score. In contrast, the human readers, who had to match each other with no artificial advantage, had only one Quadratic Weighted Kappa below 0.7, for the composite score on essay set #8 or only 1 out of 9 or 11.1%.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Besides these issues, and the ethics of eschewing a human reader&#8217;s eye, criticism of these systems has focused on the ease of gaming them, such as &lt;span class=&quot;citation&quot; data-cites=&quot;powers2002stumping&quot;&gt;@powers2002stumping&lt;/span&gt;, who managed to finagle higher scores from e-rater than humans were willing to grant (though not lower scores). Perelman himself, in response to a prompt about whether &#8220;the rising cost of a college education is the fault of students who demand [&#8230;] luxuries&#8221;, wrote an essay, excerpted below, which despite earning e-rater&#8217;s highest possible score of 6, is laden with solecisms, factual errors, and non sequiturs, including a full line of Allen Ginsberg&#8217;s &#8220;Howl&#8221; (the full essay is reproduced in &lt;a href=&quot;#appendix-a-perelmans-2012-essay&quot;&gt;Appendix A&lt;/a&gt;):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I live in a luxury dorm. In reality, it costs no more than rat infested rooms at a Motel Six. The best minds of my generation were destroyed by madness, starving hysterical naked, and publishing obscene odes on the windows of the skull. Luxury dorms pay for themselves because they generate thousand and thousands of dollars of revenue. In the Middle Ages, the University of Paris grew because it provided comfortable accommodations for each of its students, large rooms with servants and legs of mutton. Although they are expensive, these rooms are necessary to learning. The second reason for the five-paragraph theme is that it makes you focus on a single topic. Some people start writing on the usual topic, like TV commercials, and they wind up all over the place, talking about where TV came from or capitalism or health foods or whatever. But with only five paragraphs and one topic you&#8217;re not tempted to get beyond your original idea, like commercials are a good source of information about products. You give your three examples, and zap! you&#8217;re done. This is another way the five-paragraph theme keeps you from thinking too much.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!--
The question of whether computer models---including the most sophisticated ones, such as GPT-2---can understand

This is similar to a genre of [Google Translate poetry](https://languagelog.ldc.upenn.edu/nll/?cat=299) in which the same letter or syllable is repeated over and over, as in @fig:googletranslate. Having never been trained on sheer noise, the translation engine can only produce a meaningful &quot;translation&quot; of what it is fed.

![Google Translate desparately seeking order where there is only chaos. Source: [Riddled](http://eusa-riddled.blogspot.com/2017/05/sorry-for-inconvenience.html)](nu.png){#fig:googletranslate}
--&gt;
&lt;p&gt;With the above criticisms leveled, I should disclaim that I am training a model to &lt;em&gt;predict essay scores&lt;/em&gt;, not to &lt;em&gt;score essays&lt;/em&gt;, which is a much harder task (and should be held to a much higher standard) and not an obviously meaningful thing to ask of a mathematical model to begin with. However, the results show that much&#8212;even if not all&#8212;of what constitutes an essay grade is not the &lt;em&gt;je ne sais quoi&lt;/em&gt; only a human evaluator can glimpse, but rather mechanical issues that can be straightforwardly calculated and modeled.&lt;/p&gt;
&lt;h1 id=&quot;sec:dataexp&quot;&gt;Data exploration &amp;amp; cleaning&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id=&quot;essay-set-selection&quot;&gt;Essay set selection&lt;/h2&gt;
&lt;p&gt;The corpus is in the form of 13,000 essays, totaling 2.9 million words&#8212;more than twice the length of Proust&#8217;s &lt;em&gt;In Search of Lost Time&lt;/em&gt;. The length, however, was not as immediate an obstacle as the composition, shown in &lt;span class=&quot;citation&quot; data-cites=&quot;tbl:sets&quot;&gt;@tbl:sets&lt;/span&gt;. The eight essay sets were not only responding to different prompts, but were of different lengths and genres, written by students of different grade levels, and, most importantly, scored using incommensurate rubrics and scoring protocols.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;Summary of the essay sets in the ASAP corpus. &#8220;Rubric range&#8221; and &#8220;resolved range&#8221; are scores before and after adjudication, respectively. Adjudication rules have been simplified {#tbl:sets}&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Essay set&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Grade level&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Genre&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Train size&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Test size&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Avg. length&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Rubric range&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Resolved score range&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Adjudication&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;8&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Persuasion&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1,785&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;592&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;350&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1&#8211;6&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;2&#8211;12&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Sum if adjacent, else third scorer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;2&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;10&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Persuasion&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1,800&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;600&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;350&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1&#8211;6, 1&#8211;4&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1&#8211;6, 1&#8211;4&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;First&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;3&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;10&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Exposition&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1,726&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;575&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;150&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;3&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;3&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Higher if adjacent, else third scorer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;4&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;10&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Exposition&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1,772&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;589&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;150&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;3&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;3&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Higher if adjacent, else third scorer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;5&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;8&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Exposition&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1,805&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;601&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;150&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;4&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;4&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Higher&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;6&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;10&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Exposition&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1,800&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;600&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;150&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;4&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;4&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Higher&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;7&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;7&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Narrative&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1,730&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;576&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;250&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;15&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;30&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Sum&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;8&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;10&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Narrative&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;918&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;305&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;650&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;30, 0&#8211;30&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&#8211;60&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Sum if adjacent, else third scorer&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Limiting myself to a single essay set would have produced a somewhat feeble model, as words idiosyncratic to the topic in question became artificially elevated in importance. In the end, I combined sets 3 and 4, which both consisted of expository essays written by tenth graders, graded on a scale from 0 (worst) to 3 (best). These scores are holistic, i.e., not broken down into categories representing grammar and mechanics, relevance, organization, etc., which makes them easier for a model to predict.&lt;/p&gt;
&lt;h2 id=&quot;data-cleaning&quot;&gt;Data cleaning&lt;/h2&gt;
&lt;p&gt;The scores are broken down, for each essay set, into &#8220;domain scores&#8221; representing the valuations of the individual scorers. In the interest of having a single number to try to predict, I combined these scores by taking the mean:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb1&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;co&quot;&gt;# If only one score exists, use that. Otherwise, take the mean of both scores.&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-2&quot; title=&quot;2&quot;&gt;essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;score&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;list&lt;/span&gt;(&lt;span class=&quot;bu&quot;&gt;map&lt;/span&gt;(np.nanmean, &lt;span class=&quot;bu&quot;&gt;zip&lt;/span&gt;(essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;domain1_score&amp;quot;&lt;/span&gt;], essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;domain2_score&amp;quot;&lt;/span&gt;])))&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can then look at the way scores are distributed among the essays in our chosen subset.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;essays-score-hist.svg&quot; alt=&quot;Number of essays given each score&quot; id=&quot;fig:score&quot; /&gt;&lt;figcaption&gt;Number of essays given each score&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In &lt;span class=&quot;citation&quot; data-cites=&quot;fig:score&quot;&gt;@fig:score&lt;/span&gt;, we see that the scorers of the fourth essay set were somewhat less lenient than those grading the third, the latter of whom awarded the highest score to a full quarter of the papers, and the lowest score of 0 to only 39 unhappy test-takers. Putting these together, we have a roughly normal-looking distribution, with many ones and twos, and fewer zeroes and threes. This gives us a baseline to use for the modeling below: a dumb model, which assigned every essay to the plurality score class, giving every essay a score 1, would have an accuracy of 35%. This is the number our models must beat.&lt;/p&gt;
&lt;p&gt;The essays themselves are in little need of cleaning: they are hand-transcribed from the originals, and have been anonymized by replacing named entities, including names, dates, addresses, and numbers, with enumerated placeholders.&lt;/p&gt;
&lt;h2 id=&quot;data-exploration&quot;&gt;Data exploration&lt;a href=&quot;#fn2&quot; class=&quot;footnote-ref&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A basic exploration of the essays shows some striking patters. For example, as &lt;span class=&quot;citation&quot; data-cites=&quot;fig:length&quot;&gt;@fig:length&lt;/span&gt; illustrates, score is highly correlated with length at &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;R&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&#8196;=&#8196;0.51&lt;/span&gt;, meaning that over half the variation in score can be explained by variation in length. In other words, all else held equal, adding 82 words corresponds with a point increase in score.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;essay-score-length.svg&quot; alt=&quot;Length of essays (number of word tokens) by score. Box shows IQR. Whiskers show full range of data.&quot; id=&quot;fig:length&quot; /&gt;&lt;figcaption&gt;Length of essays (number of word tokens) by score. Box shows IQR. Whiskers show full range of data.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;One interesting thing we see is that, despite the correlation, there are many essays earning top marks that are almost impossibly short. The following are recorded in the dataset as having earned a top score (both prompts instructed students to use examples from the texts):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The features of the setting affect the cyclist in many ways. It made him tired thirsty and he was near exaustion [sic].&lt;a href=&quot;#fn3&quot; class=&quot;footnote-ref&quot; id=&quot;fnref3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Because she saying when the @CAPS1 grow back she will be @CAPS2 to take the test again.&lt;a href=&quot;#fn4&quot; class=&quot;footnote-ref&quot; id=&quot;fnref4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Reserved need to check keenly&lt;a href=&quot;#fn5&quot; class=&quot;footnote-ref&quot; id=&quot;fnref5&quot;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That that gnomic last &#8220;essay&#8221; (yes, that&#8217;s the whole text!) earned a coveted score of 3 is almost certainly an error, though the source of the error (the recording of the scores, the compilation of the dataset, or the scoring process itself) is as mysterious as the cryptic phrase&#8217;s meaning. However, there doesn&#8217;t seem to be an objective way of pruning these aberrant rows from the dataset, necessitating my leaving them in.&lt;/p&gt;
&lt;!--
We can try to estimate vocabulary by looking at number of word types; however, this runs into two problems.
The first is that number of types closely tracks number of tokens ($R^2=0.92$), meaning that type length is basically a proxy for token length.
The second problem is that lower-scored essays are more likely to have misspelled words, and in particular to misspell the same word multiple ways, which will inflate the type count

```python
# Type count minus misspellings
essays[&quot;types&quot;] = len(set([word.lemma_ for word in doc if is_word(word)]) &amp; wordlist)

# Type-token ratio
essays[&quot;type_ratio&quot;] = essays[&quot;types&quot;] / essays[&quot;tokens&quot;]
```
--&gt;
&lt;p&gt;Other measures are telling as well. For instance, we can look at the rate of misspelled words, by tokenizing with spaCy, and counting each token that is not in a given wordlist.&lt;a href=&quot;#fn6&quot; class=&quot;footnote-ref&quot; id=&quot;fnref6&quot;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb2&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; spacy&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-2&quot; title=&quot;2&quot;&gt;nlp &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; spacy.load(&lt;span class=&quot;st&quot;&gt;&amp;quot;en&amp;quot;&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-4&quot; title=&quot;4&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Generate wordlist&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-5&quot; title=&quot;5&quot;&gt;&lt;span class=&quot;cf&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;open&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;/usr/share/dict/words&amp;quot;&lt;/span&gt; , &lt;span class=&quot;st&quot;&gt;&amp;quot;r&amp;quot;&lt;/span&gt;) &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; infile:&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-6&quot; title=&quot;6&quot;&gt;    wordlist &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;set&lt;/span&gt;(infile.read().lower().strip().split(&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-7&quot; title=&quot;7&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-8&quot; title=&quot;8&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Number of words that are misspelled&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-9&quot; title=&quot;9&quot;&gt;essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;misspellings&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;([word &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; word &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; nlp(essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;essay&amp;quot;&lt;/span&gt;])&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-10&quot; title=&quot;10&quot;&gt;     &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;not&lt;/span&gt; word.is_space&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-11&quot; title=&quot;11&quot;&gt;     &lt;span class=&quot;kw&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;not&lt;/span&gt; word.is_punct&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-12&quot; title=&quot;12&quot;&gt;     &lt;span class=&quot;kw&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;not&lt;/span&gt; word.text.startswith(&lt;span class=&quot;st&quot;&gt;&amp;quot;@&amp;quot;&lt;/span&gt;) &lt;span class=&quot;co&quot;&gt;# named entities&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-13&quot; title=&quot;13&quot;&gt;     &lt;span class=&quot;kw&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;not&lt;/span&gt; word.text.startswith(&lt;span class=&quot;st&quot;&gt;&amp;quot;&amp;#39;&amp;quot;&lt;/span&gt;) &lt;span class=&quot;co&quot;&gt;# contractions&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-14&quot; title=&quot;14&quot;&gt;     &lt;span class=&quot;kw&quot;&gt;and&lt;/span&gt; word.text.lower() &lt;span class=&quot;kw&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; wordlist])&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-15&quot; title=&quot;15&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-16&quot; title=&quot;16&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Percentage of words misspelled&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-17&quot; title=&quot;17&quot;&gt;essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;misspellings&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;/=&lt;/span&gt; essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The results, in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:missp&quot;&gt;@fig:missp&lt;/span&gt;, are curiously complementary to those in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:length&quot;&gt;@fig:length&lt;/span&gt;: the rate of misspellings is practically the same across score classes (&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;R&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&#8196;=&#8196;0.002&lt;/span&gt;), but those at the extremes, with 10% or more of their words misspelled, are overwhelmingly likely to be low scorers.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;essay-score-misspell.svg&quot; alt=&quot;Percent of words misspelled by score. Box shows IQR. Whiskers show full range of data.&quot; id=&quot;fig:missp&quot; /&gt;&lt;figcaption&gt;Percent of words misspelled by score. Box shows IQR. Whiskers show full range of data.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The question of assessing prompt-relevance is trickier. One way of tackling it is to calculate the document vector of the story to which the students are responding, and calculate its &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;&gt;cosine similarity&lt;/a&gt; with the document vector of each essay. We can see the results in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:prompt&quot;&gt;@fig:prompt&lt;/span&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;essay-score-prompt.svg&quot; alt=&quot;Vector similarity to prompt, calculated as the cosine similarity of the mean of the word vectors in each text. Box shows IQR. Whiskers show full range of data.&quot; id=&quot;fig:prompt&quot; /&gt;&lt;figcaption&gt;Vector similarity to prompt, calculated as the cosine similarity of the mean of the word vectors in each text. Box shows IQR. Whiskers show full range of data.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The results aren&#8217;t bad (&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;R&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&#8196;=&#8196;0.28&lt;/span&gt;), especially considering the outliers for score 3 are the same bizarrely short essays we saw above, including our Delphic &#8220;reserved need to check keenly&#8221;.&lt;/p&gt;
&lt;p&gt;At this point, we must ask what the value of this metadata is. The ETS &lt;a href=&quot;https://www.ets.org/research/topics/as_nlp/writing_quality/&quot;&gt;claims&lt;/a&gt; that its e-rater accounts for prompt-relevance, as well as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;errors in grammar (e.g., subject&#8211;verb agreement)&lt;/li&gt;
&lt;li&gt;usage (e.g., preposition selection)&lt;/li&gt;
&lt;li&gt;mechanics (e.g., capitalization)&lt;/li&gt;
&lt;li&gt;style (e.g., repetitious word use)&lt;/li&gt;
&lt;li&gt;discourse structure (e.g., presence of a thesis statement, main points)&lt;/li&gt;
&lt;li&gt;vocabulary usage (e.g., relative sophistication of vocabulary)&lt;/li&gt;
&lt;li&gt;sentence variety&lt;/li&gt;
&lt;li&gt;source use&lt;/li&gt;
&lt;li&gt;discourse coherence quality&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;While it would take a sophisticated natural language parser to incorporate these details into our model, we may be able to approximate these things using metadata as proxies. Type&#8211;token ratio, for instance, could stand in for &#8220;repetitious word use&#8221;, and vector similarity to the prompt for relevance. As an alternative to parsing for narrative structure, I included a count of &#8220;linking words&#8221; that would likely signal a transition between paragraphs,&lt;a href=&quot;#fn7&quot; class=&quot;footnote-ref&quot; id=&quot;fnref7&quot;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; but this bore little relationship to the human scorers&#8217; judgments (&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;R&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&#8196;=&#8196;0.0004&lt;/span&gt;). Finally, as a proxy for sentence complexity, I used spaCy to parse the syntactic trees of each sentence, and took the longest branch, thus rewarding complex sentences with prepositional phrases and dependent clauses.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb3&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Depth of longest branch in dependency tree&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-2&quot; title=&quot;2&quot;&gt;essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;max_depth&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; [&lt;span class=&quot;bu&quot;&gt;max&lt;/span&gt;([&lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(&lt;span class=&quot;bu&quot;&gt;list&lt;/span&gt;(token.children))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-3&quot; title=&quot;3&quot;&gt;                       &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; token &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; nlp(essay)])&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-4&quot; title=&quot;4&quot;&gt;                       &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; essay &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;essay&amp;quot;&lt;/span&gt;]]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This fared somewhat better as a metric: &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;R&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&#8196;=&#8196;0.13&lt;/span&gt;. Finally, I tried to measure &#8220;relative sophistication of vocabulary&#8221; by quantifying the uncommonness of the words used. I did this by building a word frequency list from the 14-million-word &lt;a href=&quot;http://www.anc.org/&quot;&gt;American National Corpus&lt;/a&gt;, the details of which are in &lt;a href=&quot;#appendix-b-anc-wordlist&quot;&gt;Appendix B&lt;/a&gt;. This correlated well with score (&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;R&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&#8196;=&#8196;0.40&lt;/span&gt;), although it was no doubt standing in somewhat for length.&lt;/p&gt;
&lt;h1 id=&quot;modeling&quot;&gt;Modeling&lt;/h1&gt;
&lt;h2 id=&quot;sec:class&quot;&gt;Classical models&lt;a href=&quot;#fn8&quot; class=&quot;footnote-ref&quot; id=&quot;fnref8&quot;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As hinted at by the high &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;R&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt; scores above, we can get fair prediction scores by modeling on metadata alone. The first step, after splitting our essays into train and test sets, is to standardize the data by scaling to &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;z&lt;/em&gt;&lt;/span&gt;-score. I then ran principal component analysis (PCA) on the data, because many of the columns (e.g., type count and token count) encoded essentially the same information in parallel. The PCA transformation extracts those components which encode the greatest variance; together, the ten components extracted accounted for 98% of the variance within the metadata.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb4&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; train_test_split&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-2&quot; title=&quot;2&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.preprocessing &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; StandardScaler&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-3&quot; title=&quot;3&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.decomposition &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; PCA&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-4&quot; title=&quot;4&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-5&quot; title=&quot;5&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Split into train and test sets&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-6&quot; title=&quot;6&quot;&gt;X_train, X_test, y_train, y_test &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; train_test_split(X, y)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-7&quot; title=&quot;7&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-8&quot; title=&quot;8&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Standardize to z-score&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-9&quot; title=&quot;9&quot;&gt;ss &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; StandardScaler()&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-10&quot; title=&quot;10&quot;&gt;X_train_sc &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; ss.fit_transform(X_train))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-11&quot; title=&quot;11&quot;&gt;X_test_sc &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; ss.transform(X_test))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-12&quot; title=&quot;12&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-13&quot; title=&quot;13&quot;&gt;&lt;span class=&quot;co&quot;&gt;# PCA-transform&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-14&quot; title=&quot;14&quot;&gt;pca &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; PCA(n_components&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-15&quot; title=&quot;15&quot;&gt;Z_train &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pca.fit_transform(X_train_sc)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-16&quot; title=&quot;16&quot;&gt;Z_test &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pca.transform(X_test_sc)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The modeling itself is fairly straightforward. I modeled the data both with and without the PCA transform, and found the latter to have a slight edge, although all models achieved similar test scores (&lt;span class=&quot;citation&quot; data-cites=&quot;tbl:models&quot;&gt;@tbl:models&lt;/span&gt;).&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb5&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.naive_bayes &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; GaussianNB&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-2&quot; title=&quot;2&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.svm &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; SVC&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-3&quot; title=&quot;3&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.ensemble &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; AdaBoostClassifier, ExtraTreesClassifier&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-4&quot; title=&quot;4&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.metrics &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; cohen_kappa_score&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-5&quot; title=&quot;5&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-6&quot; title=&quot;6&quot;&gt;gnb &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; GaussianNB().fit(Z_train, y_train)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-7&quot; title=&quot;7&quot;&gt;svm &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; SVC(kernel&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;rbf&amp;quot;&lt;/span&gt;, C&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;).fit(Z_train, y_train)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-8&quot; title=&quot;8&quot;&gt;ext &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; ExtraTreesClassifier().fit(Z_train, y_train)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-9&quot; title=&quot;9&quot;&gt;ada &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; AdaBoostClassifier().fit(Z_train, y_train)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-10&quot; title=&quot;10&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-11&quot; title=&quot;11&quot;&gt;&lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; model &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; [gnb, svm, ext, ada]:&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-12&quot; title=&quot;12&quot;&gt;    &lt;span class=&quot;bu&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Test score:&amp;quot;&lt;/span&gt;, model.score(X_test_sc, y_test))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-13&quot; title=&quot;13&quot;&gt;    &lt;span class=&quot;bu&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;Test kappa:&amp;quot;&lt;/span&gt;, cohen_kappa_score(model.predict(X_test_sc), y_test),&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-14&quot; title=&quot;14&quot;&gt;                                           weighting&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;quadratic&amp;quot;&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I also included the weighted Cohen&#8217;s kappa, which was the metric used for the original competition, although Cohen&#8217;s kappa is typically used to compare model results to each other, not to a gold standard.&lt;/p&gt;
&lt;!--
| Train acc.   | PCA train acc.
| ------------ | ------------
| 60.7%        | 58.8%
| 67.7%        | 67.4%
| 99.9%        | 99.9%
| 60.6%        | 51.2%
--&gt;
&lt;table&gt;
&lt;caption&gt;Results of some classical models vs.&#160;35% baseline accuracy {#tbl:models}&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Test acc.&lt;/th&gt;
&lt;th&gt;PCA test acc.&lt;/th&gt;
&lt;th&gt;Test &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;&#954;&lt;/em&gt;&lt;/span&gt;&lt;/th&gt;
&lt;th&gt;PCA Test &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;&#954;&lt;/em&gt;&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;Na&#239;ve Bayes&lt;/td&gt;
&lt;td&gt;59.8%&lt;/td&gt;
&lt;td&gt;59.3%&lt;/td&gt;
&lt;td&gt;0.710&lt;/td&gt;
&lt;td&gt;0.613&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;Support vector machine&lt;/td&gt;
&lt;td&gt;65.1%&lt;/td&gt;
&lt;td&gt;63.6%&lt;/td&gt;
&lt;td&gt;0.690&lt;/td&gt;
&lt;td&gt;0.674&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;ExtraTrees&lt;/td&gt;
&lt;td&gt;63.7%&lt;/td&gt;
&lt;td&gt;62.4%&lt;/td&gt;
&lt;td&gt;0.695&lt;/td&gt;
&lt;td&gt;0.679&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;AdaBoost&lt;/td&gt;
&lt;td&gt;60.0%&lt;/td&gt;
&lt;td&gt;56.8%&lt;/td&gt;
&lt;td&gt;0.664&lt;/td&gt;
&lt;td&gt;0.670&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The support vector machine and ExtraTrees models performed slightly better than their rivals, and in fact made similar predictions to each other (&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;&#954;&lt;/em&gt;&#8196;=&#8196;0.81&lt;/span&gt;). We should also take into account that on essay sets 3 and 4, human graders agreed only about 75% of the time, with a weighted Cohen&#8217;s kappa of 0.77 and 0.85, respectively &lt;span class=&quot;citation&quot; data-cites=&quot;shermis2013contrasting&quot;&gt;[@shermis2013contrasting, p. 316]&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;recurrent-neural-network&quot;&gt;Recurrent Neural Network&lt;a href=&quot;#fn9&quot; class=&quot;footnote-ref&quot; id=&quot;fnref9&quot;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;One of the state of the art tools in text processing is the recurrent neural network, into which ordered data is fed in series, and the model is retrained on prior data, in order to learn things about the sequence. The first step to doing this with word data is to convert the words to numerical indices (so &#8220;a&#8221; becomes 1, &#8220;aardvark&#8221; becomes 2, &#8220;Aaron&#8221; becomes 3, etc.), then padding them to be of equal length.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb6&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; tensorflow.keras.preprocessing.sequence &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; pad_sequences&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-3&quot; title=&quot;3&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Define vocabulary&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-4&quot; title=&quot;4&quot;&gt;vocab &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;set&lt;/span&gt;(token.text &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; essay &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;essay&amp;quot;&lt;/span&gt;] &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; token &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; nlp.tokenizer(essay))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-5&quot; title=&quot;5&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-6&quot; title=&quot;6&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Convert words to numerical indices &amp;lt;https://www.tensorflow.org/tutorials/text/text_generation&amp;gt;&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-7&quot; title=&quot;7&quot;&gt;word2idx &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; {u: i &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; i, u &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;enumerate&lt;/span&gt;(vocab)}&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-8&quot; title=&quot;8&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-9&quot; title=&quot;9&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Convert essays to vectors of indices&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-10&quot; title=&quot;10&quot;&gt;X_vector &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; [[word2idx[token.text]&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-11&quot; title=&quot;11&quot;&gt;            &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; token &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; nlp.tokenizer(essay)]&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-12&quot; title=&quot;12&quot;&gt;            &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; essay &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; essays[&lt;span class=&quot;st&quot;&gt;&amp;quot;essay&amp;quot;&lt;/span&gt;]]&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-13&quot; title=&quot;13&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-14&quot; title=&quot;14&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Create padded sequences&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-15&quot; title=&quot;15&quot;&gt;X_vector &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pad_sequences(X_vector)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-16&quot; title=&quot;16&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-17&quot; title=&quot;17&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Split into train and test sets&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-18&quot; title=&quot;18&quot;&gt;X_vector_train, X_vector_test &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; train_test_split(X_vector)&lt;span class=&quot;op&quot;&gt;;&lt;/span&gt;&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This then goes into an embedding layer, which condenses it into a dense vector.&lt;/p&gt;
&lt;p&gt;With neural networks, it is possible to include both the vectorized document and the metadata, by processing the former in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Gated_recurrent_unit&quot;&gt;GRU&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot;&gt;LSTM&lt;/a&gt; layer, concatenating the latter to its output neurons, and processing both in a regular perceptron structure &lt;span class=&quot;citation&quot; data-cites=&quot;xing2017incorporating&quot;&gt;[See, e.g., @xing2017incorporating]&lt;/span&gt;. Following the example in &lt;a href=&quot;https://www.digital-thinking.de/deep-learning-combining-numerical-and-text-features-in-deep-neural-networks/&quot;&gt;this blog post&lt;/a&gt;, I implemented the code below:&lt;a href=&quot;#fn10&quot; class=&quot;footnote-ref&quot; id=&quot;fnref10&quot;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;!-- ![Simplified RNN schema](rnn.svg){#fig:rnn} --&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb7&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; tensorflow.keras.layers &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; Dense, GRU, Embedding, Input, Bidirectional, Concatenate&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-2&quot; title=&quot;2&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; tensorflow.keras.models &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; Model&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-4&quot; title=&quot;4&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Define inputs&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-5&quot; title=&quot;5&quot;&gt;vector_input &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; Input(shape&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;(X_vector.shape[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;],)) &lt;span class=&quot;co&quot;&gt;# Text vectors, in series of length 1,000&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-6&quot; title=&quot;6&quot;&gt;meta_input &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; Input(shape&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;(X_meta.shape[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;],)) &lt;span class=&quot;co&quot;&gt;# Scaled metadata (types, tokens, etc.)&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-7&quot; title=&quot;7&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-8&quot; title=&quot;8&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Embedding layer turns lists of word indices into dense vectors&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-9&quot; title=&quot;9&quot;&gt;rnn &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; Embedding(&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-10&quot; title=&quot;10&quot;&gt;        input_dim &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(vocab),&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-11&quot; title=&quot;11&quot;&gt;        output_dim &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;128&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-12&quot; title=&quot;12&quot;&gt;        input_length &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; X_vector.shape[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;],&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-13&quot; title=&quot;13&quot;&gt;    )(vector_input)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-14&quot; title=&quot;14&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-15&quot; title=&quot;15&quot;&gt;&lt;span class=&quot;co&quot;&gt;# GRU layers for RNN&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-16&quot; title=&quot;16&quot;&gt;rnn &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; Bidirectional(GRU(&lt;span class=&quot;dv&quot;&gt;128&lt;/span&gt;, return_sequences&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;True&lt;/span&gt;, kernel_regularizer&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;l2(&lt;span class=&quot;fl&quot;&gt;0.01&lt;/span&gt;)))(rnn)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-17&quot; title=&quot;17&quot;&gt;rnn &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; Bidirectional(GRU(&lt;span class=&quot;dv&quot;&gt;128&lt;/span&gt;, return_sequences&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;False&lt;/span&gt;, kernel_regularizer&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;l2(&lt;span class=&quot;fl&quot;&gt;0.01&lt;/span&gt;)))(rnn)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-18&quot; title=&quot;18&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-19&quot; title=&quot;19&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Incorporate metadata&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-20&quot; title=&quot;20&quot;&gt;rnn &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; Concatenate()([rnn, meta_input])&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-21&quot; title=&quot;21&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-22&quot; title=&quot;22&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Define hidden and output layers&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-23&quot; title=&quot;23&quot;&gt;rnn &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; Dense(&lt;span class=&quot;dv&quot;&gt;128&lt;/span&gt;, activation&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;, kernel_regularizer&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;l2(&lt;span class=&quot;fl&quot;&gt;0.01&lt;/span&gt;))(rnn)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-24&quot; title=&quot;24&quot;&gt;rnn &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; Dense(&lt;span class=&quot;dv&quot;&gt;128&lt;/span&gt;, activation&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;, kernel_regularizer&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;l2(&lt;span class=&quot;fl&quot;&gt;0.01&lt;/span&gt;))(rnn)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-25&quot; title=&quot;25&quot;&gt;rnn &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; Dense(&lt;span class=&quot;dv&quot;&gt;4&lt;/span&gt;, activation&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;softmax&amp;quot;&lt;/span&gt;)(rnn)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-26&quot; title=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-27&quot; title=&quot;27&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Define model&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-28&quot; title=&quot;28&quot;&gt;model &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; Model(inputs&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;[vector_input, meta_input], outputs&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;[rnn])&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-29&quot; title=&quot;29&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-30&quot; title=&quot;30&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Fit model&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-31&quot; title=&quot;31&quot;&gt;model.fit([X_vector_train, X_meta_train_sc], y_train,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-32&quot; title=&quot;32&quot;&gt;          validation_data&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;([X_vector_test, X_meta_test_sc], y_test))&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The results are surprisingly close to the models in &lt;span class=&quot;citation&quot; data-cites=&quot;sec:class&quot;&gt;@sec:class&lt;/span&gt; above. Amending our previous table:&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;Comparison of all models vs.&#160;35% baseline accuracy {#tbl:rnn}&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Test acc.&lt;/th&gt;
&lt;th&gt;Test &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;&#954;&lt;/em&gt;&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;Na&#239;ve Bayes&lt;/td&gt;
&lt;td&gt;59.8%&lt;/td&gt;
&lt;td&gt;0.710&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;Support vector machine&lt;/td&gt;
&lt;td&gt;65.1%&lt;/td&gt;
&lt;td&gt;0.690&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;ExtraTrees&lt;/td&gt;
&lt;td&gt;63.7%&lt;/td&gt;
&lt;td&gt;0.695&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;AdaBoost&lt;/td&gt;
&lt;td&gt;60.0%&lt;/td&gt;
&lt;td&gt;0.664&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;RNN&lt;/td&gt;
&lt;td&gt;63.6%&lt;/td&gt;
&lt;td&gt;0.704&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It seems that the metadata was more valuable in predicting test scores than the vectorized documents&#8212;or else, that the RNN couldn&#8217;t make better use of the two than a support vector machine could of the one. Nevertheless, I have shown that using a few key linguistic metrics, we can train a simple model to predict essay scores in fairly good agreement with human scorers.&lt;/p&gt;
&lt;h1 id=&quot;appendix-a-perelmans-2012-essay&quot; class=&quot;unnumbered&quot;&gt;Appendix A: Perelman&#8217;s (2012) essay&lt;/h1&gt;
&lt;!-- source: http://www.documentcloud.org/documents/346138-essay-awarded-a-top-grade-by-e-rater.html --&gt;
&lt;p&gt;Prompt:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Question: &#8220;The rising cost of a college education is the fault of students who demand that colleges offer students luxuries unheard of by earlier generations of college students&#8212;single dorm rooms, private bathrooms, gourmet meals, etc.&#8221;&lt;/p&gt;
&lt;p&gt;Discuss the extent to which you agree or disagree with this opinion. Support your views with specific reasons and examples from your own experience, observations, or reading.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Response:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In today&#8217;s society, college is ambiguous. We need it to live, but we also need it to love. Moreover, without college most of the world&#8217;s learning would be egregious. College, however, has myriad costs. One of the most important issues facing the world is how to reduce college costs. Some have argued that college costs are due to the luxuries students now expect. Others have argued that the costs are a result of athletics. In reality, high college costs are the result of excessive pay for teaching assistants.&lt;/p&gt;
&lt;p&gt;I live in a luxury dorm. In reality, it costs no more than rat infested rooms at a Motel Six. The best minds of my generation were destroyed by madness, starving hysterical naked, and publishing obscene odes on the windows of the skull. Luxury dorms pay for themselves because they generate thousand and thousands of dollars of revenue. In the Middle Ages, the University of Paris grew because it provided comfortable accommodations for each of its students, large rooms with servants and legs of mutton. Although they are expensive, these rooms are necessary to learning. The second reason for the five-paragraph theme is that it makes you focus on a single topic. Some people start writing on the usual topic, like TV commercials, and they wind up all over the place, talking about where TV came from or capitalism or health foods or whatever. But with only five paragraphs and one topic you&#8217;re not tempted to get beyond your original idea, like commercials are a good source of information about products. You give your three examples, and zap! you&#8217;re done. This is another way the five-paragraph theme keeps you from thinking too much.&lt;/p&gt;
&lt;p&gt;Teaching assistants are paid an excessive amount of money. The average teaching assistant makes six times as much money as college presidents. In addition, they often receive a plethora of extra benefits such as private jets, vacations in the south seas, a staring roles in motion pictures. Moreover, in the Dickens novel Great Expectation, Pip makes his fortune by being a teaching assistant. It doesn&#8217;t matter what the subject is, since there are three parts to everything you can think of. If you can&#8217;t think of more than two, you just have to think harder or come up with something that might fit. An example will often work, like the three causes of the Civil War or abortion or reasons why the ridiculous twenty-one-year-old limit for drinking alcohol should be abolished. A worse problem is when you wind up with more than three subtopics, since sometimes you want to talk about all of them.&lt;/p&gt;
&lt;p&gt;There are three main reasons while Teaching Assistants receive such high remuneration. First, they have the most powerful union in the United States. Their union is greater than the Teamsters or Freemasons, although it is slightly smaller than the international secret society of the Jedi Knights. Second, most teaching assistants have political connections, from being children of judges and governors to being the brothers and sisters of kings and princes. In Heart of Darkness, Mr.&#160;Kurtz is a teaching assistant because of his connections, and he ruins all the universities that employ him. Finally, teaching assistants are able to exercise mind control over the rest of the university community. The last reason to write this way is the most important. Once you have it down, you can use it for practically anything. Does God exist? Well, you can say yes and give three reasons, or no and give three different reasons. It doesn&#8217;t really matter. You&#8217;re sure to get a good grade whatever you pick to put into the formula. And that&#8217;s the real reason for education, to get those good grades without thinking too much and using up too much time.&lt;/p&gt;
&lt;p&gt;In conclusion, as Oscar Wilde said, &#8220;I can resist everything except temptation.&#8221; Luxury dorms are not the problem. The problem is greedy teaching assistants. It gives me an organizational scheme that looks like an essay, it limits my focus to one topic and three subtopics so I don&#8217;t wander about thinking irrelevant thoughts, and it will be useful for whatever writing I do in any subject.1 I don&#8217;t know why some teachers seem to dislike it so much. They must have a different idea about education than I do. By Les Perelman&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;appendix-b-anc-wordlist&quot; class=&quot;unnumbered&quot;&gt;Appendix B: ANC wordlist&lt;/h1&gt;
&lt;p&gt;The following code generates the &lt;a href=&quot;https://github.com/alexklapheke/essay/blob/master/data/anc_frequency_list.csv&quot;&gt;wordlist&lt;/a&gt; I used (see &lt;span class=&quot;citation&quot; data-cites=&quot;sec:dataexp&quot;&gt;@sec:dataexp&lt;/span&gt;). It took about 15 minutes to run. The ANC data is available from &lt;a href=&quot;https://www.anc.org&quot;&gt;anc.org&lt;/a&gt;, and is, per that website, &#8220;fully open and unrestricted for any use&#8221;. The resulting wordlist obeys &lt;a href=&quot;https://en.wikipedia.org/wiki/Zipf%27s_law&quot;&gt;Zipf&#8217;s law&lt;/a&gt;, as shown in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:zipf&quot;&gt;@fig:zipf&lt;/span&gt;, and is part-of-speech tagged, so homographs of different frequencies (e.g., saw&lt;sub&gt;V&lt;/sub&gt; vs.&#160;saw&lt;sub&gt;N&lt;/sub&gt;) can be distinguished.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;anc_zipf.svg&quot; alt=&quot;Demonstration of Zipf&#8217;s Law on ANC&quot; id=&quot;fig:zipf&quot; /&gt;&lt;figcaption&gt;Demonstration of Zipf&#8217;s Law on ANC&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The actual frequency measure used was the sum of word token ranks. While this gave higher results for longer sentences, and was therefore intercorrelated with token length, a very uncommon word could give the score an order-of-magnitude boost.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb8&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;co&quot;&gt;#!/usr/bin/env python3&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-3&quot; title=&quot;3&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Libraries&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-4&quot; title=&quot;4&quot;&gt;&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; glob&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-5&quot; title=&quot;5&quot;&gt;&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; spacy&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-6&quot; title=&quot;6&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; unidecode &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; unidecode&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-7&quot; title=&quot;7&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-8&quot; title=&quot;8&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Options&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-9&quot; title=&quot;9&quot;&gt;anc_path &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;/home/alex/Data/ANC/&amp;quot;&lt;/span&gt;  &lt;span class=&quot;co&quot;&gt;# freely downloadable from anc.org&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-10&quot; title=&quot;10&quot;&gt;dict_path &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;/usr/share/dict/words&amp;quot;&lt;/span&gt;  &lt;span class=&quot;co&quot;&gt;# wamerican-insane v2017.08.24-1&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-11&quot; title=&quot;11&quot;&gt;freq_per &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt; 100_000&lt;/span&gt;  &lt;span class=&quot;co&quot;&gt;# scaling factor (i.e., compute freq. per this many words)&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-12&quot; title=&quot;12&quot;&gt;include_hapaxes &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;va&quot;&gt;True&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-13&quot; title=&quot;13&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-14&quot; title=&quot;14&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Initialize spaCy&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-15&quot; title=&quot;15&quot;&gt;nlp &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; spacy.load(&lt;span class=&quot;st&quot;&gt;&amp;quot;en&amp;quot;&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-16&quot; title=&quot;16&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-17&quot; title=&quot;17&quot;&gt;freqs &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; {}&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-18&quot; title=&quot;18&quot;&gt;total_tokens &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-19&quot; title=&quot;19&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-20&quot; title=&quot;20&quot;&gt;&lt;span class=&quot;cf&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;open&lt;/span&gt;(dict_path, &lt;span class=&quot;st&quot;&gt;&amp;quot;r&amp;quot;&lt;/span&gt;) &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;file&lt;/span&gt;:&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-21&quot; title=&quot;21&quot;&gt;    dictionary &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;set&lt;/span&gt;(&lt;span class=&quot;bu&quot;&gt;file&lt;/span&gt;.read().split(&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;&lt;/span&gt;))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-22&quot; title=&quot;22&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-23&quot; title=&quot;23&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Get all text files recursively &amp;lt;https://stackoverflow.com/a/45172387&amp;gt;&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-24&quot; title=&quot;24&quot;&gt;&lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; filename &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; glob.iglob(anc_path &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;**/*.txt&amp;quot;&lt;/span&gt;, recursive&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;va&quot;&gt;True&lt;/span&gt;):&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-25&quot; title=&quot;25&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-26&quot; title=&quot;26&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Open each file in the corpus&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-27&quot; title=&quot;27&quot;&gt;    &lt;span class=&quot;cf&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;open&lt;/span&gt;(filename, &lt;span class=&quot;st&quot;&gt;&amp;quot;r&amp;quot;&lt;/span&gt;) &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;file&lt;/span&gt;:&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-28&quot; title=&quot;28&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-29&quot; title=&quot;29&quot;&gt;        &lt;span class=&quot;co&quot;&gt;# Remove diacritics, parse, &amp;amp; tokenize&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-30&quot; title=&quot;30&quot;&gt;        &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; token &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; nlp(unidecode(&lt;span class=&quot;bu&quot;&gt;file&lt;/span&gt;.read())):&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-31&quot; title=&quot;31&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-32&quot; title=&quot;32&quot;&gt;            &lt;span class=&quot;co&quot;&gt;# Eliminate non-words&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-33&quot; title=&quot;33&quot;&gt;            &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;not&lt;/span&gt; token.is_punct &lt;span class=&quot;kw&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;kw&quot;&gt;not&lt;/span&gt; token.is_space:&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-34&quot; title=&quot;34&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-35&quot; title=&quot;35&quot;&gt;                &lt;span class=&quot;co&quot;&gt;# Lemmatize and remove diacritics/ligatures&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-36&quot; title=&quot;36&quot;&gt;                lemma &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; token.lemma_.lower().strip(&lt;span class=&quot;st&quot;&gt;&amp;quot;-&amp;quot;&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-37&quot; title=&quot;37&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-38&quot; title=&quot;38&quot;&gt;                &lt;span class=&quot;co&quot;&gt;# Only use dictionary words&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-39&quot; title=&quot;39&quot;&gt;                &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; lemma &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; dictionary:&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-40&quot; title=&quot;40&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-41&quot; title=&quot;41&quot;&gt;                    &lt;span class=&quot;co&quot;&gt;# Add lemma/part-of-speech tag&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-42&quot; title=&quot;42&quot;&gt;                    type_pos &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;,&amp;quot;&lt;/span&gt;.join([lemma, token.pos_])&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-43&quot; title=&quot;43&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-44&quot; title=&quot;44&quot;&gt;                    &lt;span class=&quot;co&quot;&gt;# Update our dictionary&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-45&quot; title=&quot;45&quot;&gt;                    freqs[type_pos] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; freqs.setdefault(type_pos, &lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;) &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-46&quot; title=&quot;46&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-47&quot; title=&quot;47&quot;&gt;                    &lt;span class=&quot;co&quot;&gt;# Update our running total&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-48&quot; title=&quot;48&quot;&gt;                    total_tokens &lt;span class=&quot;op&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-49&quot; title=&quot;49&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-50&quot; title=&quot;50&quot;&gt;&lt;span class=&quot;bu&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;{:,} tokens,&amp;quot;&lt;/span&gt;.&lt;span class=&quot;bu&quot;&gt;format&lt;/span&gt;(total_tokens),&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-51&quot; title=&quot;51&quot;&gt;      &lt;span class=&quot;st&quot;&gt;&amp;quot;{:,} types&amp;quot;&lt;/span&gt;.&lt;span class=&quot;bu&quot;&gt;format&lt;/span&gt;(&lt;span class=&quot;bu&quot;&gt;len&lt;/span&gt;(freqs.keys())))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-52&quot; title=&quot;52&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-53&quot; title=&quot;53&quot;&gt;&lt;span class=&quot;co&quot;&gt;# &amp;lt;https://stackoverflow.com/a/9001529&amp;gt;&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-54&quot; title=&quot;54&quot;&gt;freqs_sorted &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;dict&lt;/span&gt;(&lt;span class=&quot;bu&quot;&gt;sorted&lt;/span&gt;(freqs.items()))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-55&quot; title=&quot;55&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-56&quot; title=&quot;56&quot;&gt;&lt;span class=&quot;cf&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;open&lt;/span&gt;(&lt;span class=&quot;st&quot;&gt;&amp;quot;anc_frequency_list.csv&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;w&amp;quot;&lt;/span&gt;) &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;file&lt;/span&gt;:&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-57&quot; title=&quot;57&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-58&quot; title=&quot;58&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# CSV header&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-59&quot; title=&quot;59&quot;&gt;    &lt;span class=&quot;bu&quot;&gt;file&lt;/span&gt;.write(&lt;span class=&quot;ss&quot;&gt;f&amp;quot;lemma,pos,count,freq_per_&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;{&lt;/span&gt;freq_per&lt;span class=&quot;sc&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;&amp;quot;&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-60&quot; title=&quot;60&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-61&quot; title=&quot;61&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# CSV rows&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-62&quot; title=&quot;62&quot;&gt;    &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; word, freq &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; freqs_sorted.items():&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-63&quot; title=&quot;63&quot;&gt;        &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; include_hapaxes &lt;span class=&quot;kw&quot;&gt;or&lt;/span&gt; freq &lt;span class=&quot;op&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;:&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-64&quot; title=&quot;64&quot;&gt;            &lt;span class=&quot;bu&quot;&gt;file&lt;/span&gt;.write(&lt;span class=&quot;ss&quot;&gt;f&amp;quot;&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;{&lt;/span&gt;word&lt;span class=&quot;sc&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;{&lt;/span&gt;freq&lt;span class=&quot;sc&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;{&lt;/span&gt;freq_per&lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;freq&lt;span class=&quot;op&quot;&gt;/&lt;/span&gt;total_tokens&lt;span class=&quot;sc&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;&amp;quot;&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/alexklapheke/essay/blob/master/code/0-Parse_data-EDA.ipynb&quot;&gt;Relevant notebook&lt;/a&gt;&lt;a href=&quot;#fnref1&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/alexklapheke/essay/blob/master/code/2-Hypothesis-tests.ipynb&quot;&gt;Relevant notebook&lt;/a&gt;&lt;a href=&quot;#fnref2&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;Essay no. 6332, set 3&lt;a href=&quot;#fnref3&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn4&quot;&gt;&lt;p&gt;Essay no. 10057, set 4&lt;a href=&quot;#fnref4&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn5&quot;&gt;&lt;p&gt;Essay no. 9870, set 4&lt;a href=&quot;#fnref5&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn6&quot;&gt;&lt;p&gt;I used &lt;a href=&quot;https://packages.ubuntu.com/bionic/wamerican-insane&quot;&gt;wamerican-insane v2017.08.24-1&lt;/a&gt;, which contains 654,749 entries.&lt;a href=&quot;#fnref6&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn7&quot;&gt;&lt;p&gt;Phrases culled from Wiktionary (&lt;a href=&quot;https://en.wiktionary.org/wiki/Category:English_conjunctive_adverbs&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://en.wiktionary.org/wiki/Category:English_sequence_adverbs&quot;&gt;2&lt;/a&gt;). The full list:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;accordingly, additionally, alphabetically, alphanumerically, also, alternatively, antepenultimately, anyway, at any rate, before, besides, by the way, chronologically, consequently, conversely, eighthly, either, eleventhly, equally, fifthly, fiftiethly, finally, first, first of all, first off, first up, firstly, for another thing, for example, for instance, for one thing, fortiethly, fourthly, further, furthermore, hence, however, hundredthly, in addition, in other words, in the first place, incidentally, indeed, lastly, likewise, moreover, neither, nevertheless, next, nextly, ninthly, nonetheless, on the contrary, on the gripping hand, on the one hand, on the other hand, otherwise, parenthetically, penultimately, rather, secondly, serially, seventhly, similarly, sixthly, sixtiethly, still, tenthly, that is, that is to say, then again, therefore, thirdly, thirteenthly, thirtiethly, though, thus, to that end, too, twelfthly, twentiethly, wherefore&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a href=&quot;#fnref7&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&quot;fn8&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/alexklapheke/essay/blob/master/code/3-Model_data.ipynb&quot;&gt;Relevant notebook&lt;/a&gt;&lt;a href=&quot;#fnref8&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn9&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/alexklapheke/essay/blob/master/code/4-Neural_net.ipynb&quot;&gt;Relevant notebook&lt;/a&gt;&lt;a href=&quot;#fnref9&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn10&quot;&gt;&lt;p&gt;The schema is, roughly:&lt;img src=&quot;rnn.svg&quot; /&gt;&lt;a href=&quot;#fnref10&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
	</description>
		</item>
		<item>
			<title>How I use org-mode to organize my recipes</title>
			<link>https://alexklapheke.github.io/blog/publish/recipes.md</link>
			<pubDate>2020-07-05T14:28:24-0400</pubDate>
	<description>
&lt;div class=&quot;epigraph&quot;&gt;
&lt;p&gt;Here the Red Queen began again. &#8220;Can you answer useful questions?&#8221; she said. &#8220;How is bread made?&#8221;&lt;/p&gt;
&lt;p&gt;&#8220;I know &lt;em&gt;that&lt;/em&gt;!&#8221; Alice cried eagerly. &#8220;You take some flour&#8212;&#8221;&lt;/p&gt;
&lt;p&gt;&#8220;Where do you pick the flower?&#8221; the White Queen asked. &#8220;In a garden, or in the hedges?&#8221;&lt;/p&gt;
&lt;p&gt;&#8220;Well, it isn&#8217;t &lt;em&gt;picked&lt;/em&gt; at all,&#8221; Alice explained: &#8220;it&#8217;s &lt;em&gt;ground&lt;/em&gt;&#8212;&#8221;&lt;/p&gt;
&lt;p&gt;&#8220;How many acres of ground?&#8221; said the White Queen. &#8220;You mustn&#8217;t leave out so many things.&#8221;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Cooking is a major hobby of mine, and for a long time, I kept recipes either in text files or printed PDFs in a poorly organized folder on my hard drive. It was hard to find specific recipes (did I file &lt;a href=&quot;https://www.tastemade.com/shows/thirsty-for/mexican-horchata&quot;&gt;horchata&lt;/a&gt; under &#8220;Drinks&#8221; or &#8220;Mexican&#8221;?), hard to remember whether I had made a recipe before (and whether it was any good), and pretty much impossible to search for complicated things like &#8220;recipes without seafood&#8221; or &#8220;recipes that take less than an hour&#8221;. To make things worse, the whole folder was cluttered with useless infographics and spreadsheets.&lt;/p&gt;
&lt;p&gt;I recently discovered &lt;a href=&quot;https://orgmode.org/&quot;&gt;org-mode&lt;/a&gt;, an organization system for the &lt;a href=&quot;https://www.gnu.org/software/emacs/&quot;&gt;Emacs&lt;/a&gt; text editor, and while there is a bit of a learning curve, it is the best organization tool I&#8217;ve ever used. I took a few days last winter to wrangle my recipe collection into something manageable. I had the following criteria for my setup:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Recipes can easily be copied from websites without adding a lot of formatting/structure.&lt;/li&gt;
&lt;li&gt;Recipes can be searched or filtered based on common metadata, such as yield or cooking time.&lt;/li&gt;
&lt;li&gt;There is a space to add &#8220;cooking notes&#8221;, so the next time I make the recipe I&#8217;ll remember what happened the first time (say, substitutions I made or steps that were unclear).&lt;/li&gt;
&lt;li&gt;It is obvious where any particular recipe should be placed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After playing with some possible schemes, I settled on the one below.&lt;/p&gt;
&lt;h1 id=&quot;file-structure&quot;&gt;File structure&lt;/h1&gt;
&lt;p&gt;All my recipes are in the single file &lt;code&gt;Cooking.org&lt;/code&gt;, the basic structure of which is below.&lt;/p&gt;
&lt;pre class=&quot;org-mode&quot;&gt;&lt;code&gt;#+TITLE: Recipes
#+STARTUP: indent align overview
#+TODO: NEVERMADE(n) | HAVEMADE(h@)
#+TAGS: { weekday weekend } { vegetarian seafood } glutenfree breakfast instantpot slowcooker
#+TAGS: { { asian : chinese(c) japanese(j) korean(k) vietnamese(v) } lebanese(l) indian(i) }

* Meals:
** Meats:
** Vegetables, stir-fries, &amp;amp; curries:
** Rice/orzo &amp;amp; grains:
** Pasta &amp;amp; noodles:
** Soups, stews, &amp;amp; casseroles:
** Salads:
** Sandwiches:
** Breakfast eggs &amp;amp; grains:
* Condiments &amp;amp; snacks:
** Pickles &amp;amp; preserves:
** Sauces &amp;amp; toppings:
** Snacks &amp;amp; dips:
* Baked goods:
** Savory breads/rolls:
** Sweet breads/rolls:
** Batters:
** Cakes &amp;amp; tortes:
** Pies, tarts, &amp;amp; cobblers:
** Pastries, custards/curds, &amp;amp; ice cream:
** Cookies:
* Drinks:
** Hot drinks:
** Cold drinks:
* Projects:
** Brewing:
** Dairy:
** Bread &amp;amp; noodles:
** Candy:
* Notes:
** Price book:
** Unsorted recipes:
** Settings:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first few lines, starting with &lt;code&gt;#+&lt;/code&gt;, make up the file header. The &lt;code&gt;#+STARTUP&lt;/code&gt; line defines some &lt;a href=&quot;https://orgmode.org/manual/In_002dbuffer-settings.html#index-_0023_002bSTARTUP&quot;&gt;settings&lt;/a&gt; for when I open the file. The most important is &lt;code&gt;overview&lt;/code&gt;, which makes sure only the headings are shown, and not all ~30,000 lines of text. Unfortunately, this only shows top-level headings. I can use &lt;kbd&gt;M-2 S-Tab&lt;/kbd&gt;&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; to see the second-level headings as well, or &lt;kbd&gt;M-3 S-Tab&lt;/kbd&gt; to see all recipes, although at this point, I have far too many for the latter to be useful.&lt;/p&gt;
&lt;p&gt;I also define &lt;a href=&quot;https://orgmode.org/manual/Tags.html&quot;&gt;tags&lt;/a&gt; and &lt;a href=&quot;https://orgmode.org/manual/TODO-Items.html&quot;&gt;TODO states&lt;/a&gt;, the latter of which let you mark an item as being in any of various states of completion. The &lt;code&gt;@&lt;/code&gt; after the &lt;code&gt;HAVEMADE&lt;/code&gt; state means that when I mark a recipe as &lt;code&gt;HAVEMADE&lt;/code&gt;, org should prompt me to &lt;a href=&quot;https://orgmode.org/manual/Tracking-TODO-state-changes.html&quot;&gt;leave a note&lt;/a&gt;. Thus, after I make a recipe, I can jot down what went well and what I should change next time. The tags define ancillary properties of the recipe&#8212;whether it is gluten-free, what part of the world it comes from&#8212;and tags surrounded with curly braces are mutually exclusive, such as &lt;code&gt;{ vegetarian seafood }&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the body of the file, the recipes are organized into first- and second-level &lt;a href=&quot;https://orgmode.org/manual/Headlines.html&quot;&gt;headings&lt;/a&gt;, starting with &lt;code&gt;*&lt;/code&gt; and &lt;code&gt;**&lt;/code&gt;, respectively. The tacit rule is that recipes within a category should substitute for each other&#8212;thus, &lt;code&gt;Cakes &amp;amp; tortes&lt;/code&gt; is a valid category because &lt;a href=&quot;https://smittenkitchen.com/2015/12/tres-leches-cake-a-taco-party/&quot;&gt;tres leches cake&lt;/a&gt; could substitute for &lt;a href=&quot;https://www.americastestkitchen.com/recipes/9648-olive-oil-cake&quot;&gt;olive oil cake&lt;/a&gt;; but it couldn&#8217;t substitute for &lt;a href=&quot;https://www.budgetbytes.com/pickled-red-onions/&quot;&gt;pickled red onions&lt;/a&gt;, so it wouldn&#8217;t make sense to group them under a &lt;code&gt;Mexican&lt;/code&gt; heading.&lt;/p&gt;
&lt;!-- This removes nearly all ambiguity from the sorting/searching process and makes it extremely easy to search recipes. --&gt;
&lt;!-- For example, I can find meatless recipes for my vegetarian friends, or if I&apos;m making a [Japanese entr&#233;e](https://www.seriouseats.com/recipes/2018/01/japanese-curry-kare.html), I can easily find a [Japanese side dish](https://cooking.nytimes.com/recipes/1015648-simple-seaweed-salad) to complement it. --&gt;
&lt;p&gt;The final top-level heading is &lt;code&gt;Notes&lt;/code&gt;. The first thing here is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Price_book&quot;&gt;price book&lt;/a&gt;&#8212;a simple table of ingredients I buy often, and at which nearby grocery stores they are cheapest, with the cheapest price in bold.&lt;a href=&quot;#fn2&quot; class=&quot;footnote-ref&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; I also rounded up to the nearest dime to make mental math easier, and combat stores&#8217; &lt;a href=&quot;https://en.wikipedia.org/wiki/Psychological_pricing&quot;&gt;psychological tricks&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&quot;org-mode&quot;&gt;&lt;code&gt;** Price book:
- Rules of thumb:
  - Apple/peach/onion :: 1/2lb ($1/ @ $2/lb, $1.50/ @ $3/lb)
  - Banana :: 1/3lb (15&#162;/ @ 50&#162;/lb)
  - Head of garlic :: 1/8lb
  - Chicken drumstick :: 7oz
| Food               | Trader Joe&amp;#39;s | Whole Foods |
|--------------------+--------------+-------------|
| Milk (1/2gal)      | *$1.70*      | $2.20       |
| Eggs (dozen large) | *$2*         | $3          |
| Butter (4 sticks)  | *$3*         | $3.50       |
| Coffee (12oz)      | $4           | $4          |
| Garlic (head)      | 70&#162;          | *~50&#162;*      |
| Lemon              | 50&#162;          | 50&#162;         |&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second subheading under &lt;code&gt;Notes&lt;/code&gt; is &lt;code&gt;Unsorted Recipes&lt;/code&gt;, where recipes are automatically placed when added (see &lt;a href=&quot;#adding-recipes&quot;&gt;below&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Finally, a &lt;code&gt;Settings&lt;/code&gt; subheading contains the &lt;a href=&quot;https://www.gnu.org/software/emacs/manual/html_node/emacs/Specifying-File-Variables.html&quot;&gt;in-file settings&lt;/a&gt;&#8212;this must be the last thing in the file. The first line sets the &lt;a href=&quot;https://orgmode.org/manual/Refile-and-Copy.html&quot;&gt;refile&lt;/a&gt; targets to the second-level headings, so recipes can easily be moved between categories while keeping them at the third heading level. The second line sets &lt;a href=&quot;https://orgmode.org/manual/Setting-tags.html&quot;&gt;&lt;code&gt;org-tags-column&lt;/code&gt;&lt;/a&gt; to &lt;code&gt;-105&lt;/code&gt; from the default value of &lt;code&gt;-77&lt;/code&gt;. I have a lot of tags on recipes, and this reduces visual clutter by moving them farther to the right.&lt;/p&gt;
&lt;pre class=&quot;org-mode&quot;&gt;&lt;code&gt;** Settings:
# Local variables:
# org-refile-targets: ((&amp;quot;~/org/Cooking.org&amp;quot; :level . 2))
# org-tags-column: -105
# End:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a final bonus, this structure makes it easy to see how many recipes I have: hit &lt;kbd&gt;M-!&lt;/kbd&gt; to get a shell prompt, then run &lt;code class=&quot;sourceCode bash&quot;&gt;&lt;span class=&quot;fu&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;^\*\*\*&amp;#39;&lt;/span&gt; Cooking.org &lt;span class=&quot;kw&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;wc&lt;/span&gt; -l&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&quot;recipe-structure&quot;&gt;Recipe structure&lt;/h1&gt;
&lt;p&gt;The third level is for the recipes themselves. These basically consist of a &lt;a href=&quot;https://orgmode.org/manual/Property-syntax.html&quot;&gt;properties drawer&lt;/a&gt; for the metadata, a &lt;a href=&quot;https://orgmode.org/manual/Checkboxes.html&quot;&gt;checklist&lt;/a&gt; for the ingredients, and a &lt;a href=&quot;https://orgmode.org/manual/Plain-lists.html&quot;&gt;numbered list&lt;/a&gt; for the directions.&lt;/p&gt;
&lt;p&gt;For example, here is a recipe for marinated eggs: the properties drawer runs from &lt;code&gt;:PROPERTIES:&lt;/code&gt; to &lt;code&gt;:END:&lt;/code&gt;, followed by the ingredients prepended with &lt;code&gt;- [ ]&lt;/code&gt;, then the directions and an endnote.&lt;/p&gt;
&lt;pre class=&quot;org-mode&quot;&gt;&lt;code&gt;*** HAVEMADE Japanese Marinated Soft Boiled Egg for Ramen (Ajitsuke Tamago)          :weekend:japanese:
:PROPERTIES:
:Author: J. Kenji L&#243;pez-Alt
:Source:
:Sent_by:
:Yield: 6 eggs
:Prep_Time: 0:10
:Cook_Time: 4:00
:Total_Time: 4:10
:Cost:
:Description: Perfectly seasoned soft-boiled eggs for the best homemade ramen.
:URL: https://www.seriouseats.com/recipes/2012/03/ajitsuke-tamago-japanese-marinated-soft-boiled-egg-recipe.html
:Added: [2019-01-03 Thu]
:END:
- [ ] 1 cup water
- [ ] 1 cup sake
- [ ] 1/2 cup soy sauce
- [ ] 1/2 cup mirin
- [ ] 1/2 cup sugar
- [ ] 6 eggs


1. Combine water, sake, soy, mirin, and sugar in a medium bowl and
   whisk until sugar is dissolved. Set aside.

2. Bring 2 quarts of water to a boil in a medium saucepan over high
   heat. Pierce fat end of each egg with a thumbtack to make a tiny
   hole (this prevents them from cracking and eliminates the air
   bubble at the end). Carefully lower eggs into water with a wire
   mesh spider or slotted spoon. Reduce heat to maintain a bare
   simmer. Cook for exactly 6 minutes. Drain hot water and carefully
   peel eggs under cold running water (the whites will be quite
   delicate).

3. Transfer eggs to a bowl that just barely fits them all. Pour
   marinade on top until eggs are covered or just floating. Place a
   double-layer of paper towels on top and press down until completely
   saturated in liquid to help keep eggs submerged and marinating
   evenly. Refrigerate and marinate at least four hours and up to 12.
   Discard marinade after 12 hours. Store eggs in a sealed container
   in the fridge for up to 3 days. Reheat in ramen soup to serve.

Note: This recipe can be made using leftover broth from chashu pork.
If you have this broth, replace all the ingredients in the marinade
with the broth.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ingredients list is interactive: with the cursor on one of the list items, I can use &lt;code&gt;C-c C-c&lt;/code&gt; to check or uncheck it. This is extremely useful for making shopping lists. The directions list has to be separated by &lt;em&gt;two&lt;/em&gt; blank lines from the ingredients list, or org-mode will parse them as being part of the same list, and insist they both be bullets or both be numbers.&lt;/p&gt;
&lt;p&gt;The properties drawer has slots for most of the metadata that comes with a recipe: author, yield, estimated time, etc., as well as one for the URL, and the date it was added to &lt;code&gt;Cooking.org&lt;/code&gt;. Combined with the tag structure, it is a breeze to search for recipes by hitting &lt;kbd&gt;C-c / m&lt;/kbd&gt; and typing in a &lt;a href=&quot;https://orgmode.org/manual/Matching-tags-and-properties.html&quot;&gt;match pattern&lt;/a&gt;. This creates a &lt;a href=&quot;https://orgmode.org/manual/Sparse-Trees.html&quot;&gt;sparse tree&lt;/a&gt;, which shows the titles of all matching recipes. For example, I can search for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chinese recipes with the pattern &lt;code&gt;chinese&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Chinese recipes that are &lt;em&gt;also&lt;/em&gt; vegetarian with &lt;code&gt;chinese+vegetarian&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Vegetarian Chinese recipes I&#8217;ve never made before with &lt;code&gt;chinese+vegetarian/NEVERMADE&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Recipes I&#8217;ve added in the past month with &lt;code&gt;Added&amp;gt;=&amp;quot;&amp;lt;-1m&amp;gt;&amp;quot;&lt;/code&gt;, or since May with &lt;code&gt;Added&amp;gt;=&amp;quot;&amp;lt;2020-05-01&amp;gt;&amp;quot;&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Recipes that come together in less than 90 minutes with &lt;code&gt;Total_time&amp;lt;&amp;quot;1:30&amp;quot;&lt;/code&gt;;&lt;a href=&quot;#fn3&quot; class=&quot;footnote-ref&quot; id=&quot;fnref3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recipes from the New York Times with &lt;code&gt;URL={cooking.nytimes.com}&lt;/code&gt;. The braces indicate a regular expression.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also the &lt;kbd&gt;C-c / p&lt;/kbd&gt; command for simple equality matches&#8212;for example, if I wanted to find all recipes by &lt;a href=&quot;https://en.wikipedia.org/wiki/Amanda_Hesser&quot;&gt;Amanda Hesser&lt;/a&gt;&#8212;which supports tab completion.&lt;/p&gt;
&lt;h1 id=&quot;adding-recipes&quot;&gt;Adding recipes&lt;/h1&gt;
&lt;p&gt;Although adding a new recipe is as easy as creating a new heading, it would be tedious to copy the properties drawer each time. A much easier way is to use &lt;a href=&quot;https://orgmode.org/manual/Capture.html&quot;&gt;capture&lt;/a&gt;, which allows the quick &#8220;capturing&#8221; and filing of data from any open file. I keep the following &lt;a href=&quot;https://orgmode.org/manual/Capture-templates.html&quot;&gt;capture template&lt;/a&gt; in my &lt;code&gt;~/.emacs&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb5&quot;&gt;&lt;pre class=&quot;sourceCode lisp&quot;&gt;&lt;code class=&quot;sourceCode commonlisp&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-1&quot; title=&quot;1&quot;&gt;(custom-set-variables&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-2&quot; title=&quot;2&quot;&gt; &amp;#39;(org-capture-templates&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-3&quot; title=&quot;3&quot;&gt;    &amp;#39;((&lt;span class=&quot;st&quot;&gt;&amp;quot;r&amp;quot;&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;Recipe&amp;quot;&lt;/span&gt; entry&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-4&quot; title=&quot;4&quot;&gt;      (file+headline &lt;span class=&quot;st&quot;&gt;&amp;quot;~/org/Cooking.org&amp;quot;&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;Unsorted recipes:&amp;quot;&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-5&quot; title=&quot;5&quot;&gt;      &lt;span class=&quot;st&quot;&gt;&amp;quot;* NEVERMADE %? :weekday:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-6&quot; title=&quot;6&quot;&gt;&lt;span class=&quot;st&quot;&gt;:PROPERTIES:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-7&quot; title=&quot;7&quot;&gt;&lt;span class=&quot;st&quot;&gt;:Author:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-8&quot; title=&quot;8&quot;&gt;&lt;span class=&quot;st&quot;&gt;:Source:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-9&quot; title=&quot;9&quot;&gt;&lt;span class=&quot;st&quot;&gt;:Sent_by:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-10&quot; title=&quot;10&quot;&gt;&lt;span class=&quot;st&quot;&gt;:Yield:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-11&quot; title=&quot;11&quot;&gt;&lt;span class=&quot;st&quot;&gt;:Prep_Time:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-12&quot; title=&quot;12&quot;&gt;&lt;span class=&quot;st&quot;&gt;:Cook_Time:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-13&quot; title=&quot;13&quot;&gt;&lt;span class=&quot;st&quot;&gt;:Total_Time:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-14&quot; title=&quot;14&quot;&gt;&lt;span class=&quot;st&quot;&gt;:Cost:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-15&quot; title=&quot;15&quot;&gt;&lt;span class=&quot;st&quot;&gt;:Description:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-16&quot; title=&quot;16&quot;&gt;&lt;span class=&quot;st&quot;&gt;:URL:&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-17&quot; title=&quot;17&quot;&gt;&lt;span class=&quot;st&quot;&gt;:Added: %u&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-18&quot; title=&quot;18&quot;&gt;&lt;span class=&quot;st&quot;&gt;:END:&amp;quot;&lt;/span&gt;))))&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I can add a recipe from any file by hitting &lt;kbd&gt;C-c c r&lt;/kbd&gt;; org-mode will create a new recipe under &lt;code&gt;Unsorted Recipes&lt;/code&gt; with the template above, and I just have to fill in the details. The &lt;code&gt;%u&lt;/code&gt; is &lt;a href=&quot;https://orgmode.org/manual/Template-expansion.html&quot;&gt;automatically replaced&lt;/a&gt; with the current date, and the &lt;code&gt;%?&lt;/code&gt; tells org-mode where to put the cursor. Since most recipes follow the same basic pattern (paragraph of metadata, unnumbered list of ingredients, numbered list of directions), formatting the recipes as described above is very quick, with a few caveats: times have to be formatted as &lt;code&gt;HH:MM&lt;/code&gt;, and the description can&#8217;t run into multiple paragraphs (I use &lt;code&gt;//&lt;/code&gt; as a paragraph separator, but I&#8217;m not thrilled with this solution).&lt;/p&gt;
&lt;p&gt;Finally, the new recipe can be easily refiled with &lt;kbd&gt;C-c C-w&lt;/kbd&gt;.&lt;/p&gt;
&lt;h1 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h1&gt;
&lt;p&gt;The main downside to this system is that there is no easy way to store images. Many recipes have &lt;a href=&quot;https://smittenkitchen.com/2012/03/potato-knish-two-ways/&quot;&gt;complicated assemblies&lt;/a&gt; that are best explained in pictures. The best I can do is store a note that says something like &#8220;see pictures in original recipe&#8221;, which has been good enough so far.&lt;/p&gt;
&lt;!--
I&apos;ll end with another upside though; namely, that I still have those charts that were cluttering up the folder, but now they&apos;re actually useful since they&apos;re the only things in there.
Here&apos;s one I made for the [mother sauces](https://en.wikipedia.org/wiki/Sauce#French_cuisine) (click to download PDF):

[![](MotherSauces.png)](MotherSauces.pdf)
--&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;In Emacs-speak, &lt;kbd&gt;C&lt;/kbd&gt; is the control key, &lt;kbd&gt;M&lt;/kbd&gt;, or &#8220;meta&#8221; is alt, and &lt;kbd&gt;S&lt;/kbd&gt; is shift. So &lt;kbd&gt;M-2 S-Tab&lt;/kbd&gt; means you hit alt-2, then shift-tab in sequence.&lt;a href=&quot;#fnref1&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;This is just a subset for illustration; my actual table is larger.&lt;a href=&quot;#fnref2&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;In this and the above example, notice the quotation marks.&lt;a href=&quot;#fnref3&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
	</description>
		</item>
		<item>
			<title>Menu Categorization</title>
			<link>https://alexklapheke.github.io/blog/publish/menu.md</link>
			<pubDate>2020-07-03T18:11:59-0400</pubDate>
	<description>
&lt;div class=&quot;epigraph&quot;&gt;
&lt;p&gt;The History of every major Galactic Civilization tends to pass through three distinct and recognizable phases, those of Survival, Inquiry and Sophistication, otherwise known as the How, Why and Where phases.&lt;/p&gt;
&lt;p&gt;For instance, the first phase is characterized by the question &#8220;How can we eat?&#8221;, the second by the question &#8220;Why do we eat?&#8221; and the third by the question, &#8220;Where shall we have lunch?&#8221;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Since 2011, the New York Public Library has maintained &lt;a href=&quot;http://menus.nypl.org/&quot;&gt;&#8220;What&#8217;s on the menu?&#8221;&lt;/a&gt;, a collection of tens of thousands of restaurant menus going back to the mid-nineteenth century. This is an invaluable collection not just for data scientists, but for food historians, since it is well known that foods go in and out of fashion like clothing. For example, the famed &lt;a href=&quot;https://en.wikipedia.org/wiki/Grand_Central_Oyster_Bar_%26_Restaurant&quot;&gt;Oyster Bar&lt;/a&gt; at Grand Central Terminal, which in 1941 &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Grand_Central_Terminal_Restaurant_menu_1941.jpg&quot;&gt;featured&lt;/a&gt; &#8220;cream of chicken &#224; la reine&#8221;, &#8220;broiled sweetbreads on toast with Virginia ham&#8221;, and &#8220;farina custard pudding, Melba sauce&#8221; (not to mention oysters for a nickel apiece), today &lt;a href=&quot;https://web.archive.org/web/20160315171449/http://www.oysterbarny.com/pdf/dailymenu.pdf&quot;&gt;serves&lt;/a&gt; such mouthfuls as &#8220;poached farmed Norwegian salmon over baby red oak-watercress salad with charred scallion-honey vinaigrette, avocado, and goat cheese.&#8221;&lt;/p&gt;
&lt;p&gt;These vicissitudes are due partly to economics. In 1914, avocados, then known as &#8220;alligator pears&#8221;, &lt;a href=&quot;https://time.com/4832655/avocado-american-history/&quot;&gt;could go for&lt;/a&gt; $1 each&#8212;more that $25 today. But economics can be adulterated by public perception. David Foster Wallace, considering the lobster, wrote that &lt;span class=&quot;citation&quot; data-cites=&quot;wallace2004consider&quot;&gt;[-@wallace2004consider,p. 55]&lt;/span&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Up until sometime in the 1800s, though, lobster was literally low-class food, eaten only by the poor and institutionalized. Even in the harsh penal environment of early America, some colonies had laws against feeding lobsters to inmates more than once a week because it was thought to be cruel and unusual, like making people eat rats. One reason for their low status was how plentiful lobsters were in old New England. &#8220;Unbelievable abundance&#8221; is how one source describes the situation, including accounts of Plymouth pilgrims wading out and capturing all they wanted by hand, and of early Boston&#8217;s seashore being littered with lobsters after hard storms&#8212;these latter were treated as a smelly nuisance and ground up for fertilizer. There is also the fact that premodern lobster was often cooked dead and then preserved, usually packed in salt or crude hermetic containers. Maine&#8217;s earliest lobster industry was based around a dozen such seaside canneries in the 1840s, from which lobster was shipped as far away as California, in demand only because it was cheap and high in protein, basically chewable fuel.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By 1941, of course, the Oyster Bar&#8217;s menu lists &#8220;alligator pear salad&#8221; for 45&#162; and &#8220;lobster pan roast&#8221;&#8212;one of the priciest items named&#8212;for $1.45.&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; In this project I wanted to see if, using this data,&lt;a href=&quot;#fn2&quot; class=&quot;footnote-ref&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; I could predict the year a menu was served based on the dishes listed.&lt;/p&gt;
&lt;h1 id=&quot;data-structuring&quot;&gt;Data structuring&lt;/h1&gt;
&lt;p&gt;The NYPL provides the data in the very simple &lt;a href=&quot;https://en.wikipedia.org/wiki/Snowflake_schema&quot;&gt;snowflake schema&lt;/a&gt; shown in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:schema&quot;&gt;@fig:schema&lt;/span&gt;. The central table is &lt;code&gt;MenuItem.csv&lt;/code&gt;, in which each of the 1,334,417 rows represents an item on a menu. Each references a particular dish, which are named in &lt;code&gt;Dish.csv&lt;/code&gt; (426,959 comestibles in total), and each is also referenced to a page in &lt;code&gt;MenuPage.csv&lt;/code&gt;, which are in turn referenced in &lt;code&gt;Menu.csv&lt;/code&gt; to the particular bills of fare on which they appear.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-schema.svg&quot; alt=&quot;Menu dataset table schema (some columns omitted). Source code&quot; id=&quot;fig:schema&quot; /&gt;&lt;figcaption&gt;Menu dataset table schema (some columns omitted). &lt;a href=&quot;https://gist.github.com/alexklapheke/a197eed2a9ba742aa0080c1bdbfa579c&quot;&gt;Source code&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Since all I wanted was the name of the dish (from &lt;code&gt;Dish.csv&lt;/code&gt;), the date (from &lt;code&gt;Menu.csv&lt;/code&gt;), and the menu ID (in case I wanted to group dishes by menu), I merged the data frames like so:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb1&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Add menu id to each menu item&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-2&quot; title=&quot;2&quot;&gt;df &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.merge(&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-3&quot; title=&quot;3&quot;&gt;    left &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; menu_item[[&lt;span class=&quot;st&quot;&gt;&amp;quot;dish_id&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;menu_page_id&amp;quot;&lt;/span&gt;]],&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-4&quot; title=&quot;4&quot;&gt;    right &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; menu_page[[&lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;menu_id&amp;quot;&lt;/span&gt;]],&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-5&quot; title=&quot;5&quot;&gt;    how &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;right&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-6&quot; title=&quot;6&quot;&gt;    left_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;menu_page_id&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-7&quot; title=&quot;7&quot;&gt;    right_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-8&quot; title=&quot;8&quot;&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-9&quot; title=&quot;9&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-10&quot; title=&quot;10&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Add menu date to each menu id&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-11&quot; title=&quot;11&quot;&gt;df &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.merge(&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-12&quot; title=&quot;12&quot;&gt;    left &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-13&quot; title=&quot;13&quot;&gt;    right &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; menu[[&lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;date&amp;quot;&lt;/span&gt;]],&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-14&quot; title=&quot;14&quot;&gt;    how &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;right&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-15&quot; title=&quot;15&quot;&gt;    left_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;menu_id&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-16&quot; title=&quot;16&quot;&gt;    right_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-17&quot; title=&quot;17&quot;&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-18&quot; title=&quot;18&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-19&quot; title=&quot;19&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Add dish name to each menu item&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-20&quot; title=&quot;20&quot;&gt;df &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.merge(&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-21&quot; title=&quot;21&quot;&gt;    left &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-22&quot; title=&quot;22&quot;&gt;    right &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; dish[[&lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;]],&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-23&quot; title=&quot;23&quot;&gt;    how &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;right&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-24&quot; title=&quot;24&quot;&gt;    left_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;dish_id&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-25&quot; title=&quot;25&quot;&gt;    right_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-26&quot; title=&quot;26&quot;&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-27&quot; title=&quot;27&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-28&quot; title=&quot;28&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Remove intermediate columns&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-29&quot; title=&quot;29&quot;&gt;df &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;date&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;menu_id&amp;quot;&lt;/span&gt;]]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This left me with a single data frame of menu items to clean and parse.&lt;/p&gt;
&lt;h1 id=&quot;data-cleaning&quot;&gt;Data cleaning&lt;/h1&gt;
&lt;h2 id=&quot;dates&quot;&gt;Dates&lt;/h2&gt;
&lt;p&gt;The first work to be done was on the dates. For example, 638 dates turned out to be incorrect or malformed. In some, like &lt;code&gt;1091-01-27&lt;/code&gt;, the error was transparent, but with less than 0.05% of the data so corrupted, I decided to just drop them. However, another 68,438 items&#8212;5% of the data&#8212;were missing dates altogether. Since they were useless to the analysis, and I couldn&#8217;t know if there was a pattern to the missingness, I was forced to drop these as well. Then, from the 1.27 million well-formed dates remaining, I extracted the year and decade, the latter of which would prove a more reasonable target for modeling than the former.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb2&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Drop malformed dates&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-2&quot; title=&quot;2&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;date&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.to_datetime(df[&lt;span class=&quot;st&quot;&gt;&amp;quot;date&amp;quot;&lt;/span&gt;], errors&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;coerce&amp;quot;&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-4&quot; title=&quot;4&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Calculate year and decade&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-5&quot; title=&quot;5&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;year&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[&lt;span class=&quot;st&quot;&gt;&amp;quot;date&amp;quot;&lt;/span&gt;].dt.year&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-6&quot; title=&quot;6&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;decade&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[&lt;span class=&quot;st&quot;&gt;&amp;quot;year&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A further problem is that our dataset&#8217;s classes are heavily unbalanced. Looking at &lt;span class=&quot;citation&quot; data-cites=&quot;fig:decade&quot;&gt;@fig:decade&lt;/span&gt;, we see that the great preponderance of them&#8212;a full 63% of menus and 62% of items&#8212;are from the initial two decades of the twentieth century.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-items-by-decade.svg&quot; alt=&quot;Number of menus (left) and menu items (right) by decade&quot; id=&quot;fig:decade&quot; /&gt;&lt;figcaption&gt;Number of menus (left) and menu items (right) by decade&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This could cause problems during modeling, not only because a na&#239;ve model could latch onto the majority class and return it without considering the inputs, but because the meagerness of data&#8212;particularly before 1880 and after 1990&#8212;will leave the model with inadequate information to categorize &lt;em&gt;any&lt;/em&gt; menu as being from these eras.&lt;/p&gt;
&lt;h2 id=&quot;menus&quot;&gt;Menus&lt;/h2&gt;
&lt;p&gt;Looking at the number of items featured on each menu, we see that although 22.7% of them feature fewer than a hundred items, several reach into the thousands, with a startling outlier:&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-item-length.svg&quot; alt=&quot;Number of items on menus (log10 scale)&quot; id=&quot;fig:items&quot; /&gt;&lt;figcaption&gt;Number of items on menus (log&lt;sub&gt;10&lt;/sub&gt; scale)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;That last menu, a &lt;a href=&quot;http://menus.nypl.org/menus/31054&quot;&gt;1933 Plaza Hotel menu&lt;/a&gt;, is actually 62 menus bound together, totalling 4,060 dishes. However, even the runner-up, a &lt;a href=&quot;http://menus.nypl.org/menus/34201&quot;&gt;1914 Waldorf Astoria menu&lt;/a&gt;, packs 1,360 dishes into ten octavo-sized pages. (For reference, the famously voluminous Cheesecake Factory menu has &lt;a href=&quot;https://www.mentalfloss.com/article/566482/why-the-cheesecake-factorys-menu-is-so-big&quot;&gt;&#8220;only&#8221;&lt;/a&gt; 250 items.) This shouldn&#8217;t matter to the model, of course, since it will be trained only on individual dish names.&lt;/p&gt;
&lt;h2 id=&quot;dishes&quot;&gt;Dishes&lt;/h2&gt;
&lt;p&gt;The dish names themselves were somewhat less tractable. The menus are transcribed by hand, eliminating the need to deal with OCR errors, but many items were unreasonably long.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-name-length.svg&quot; alt=&quot;Character length of menu items (log10 scale)&quot; id=&quot;fig:length&quot; /&gt;&lt;figcaption&gt;Character length of menu items (log&lt;sub&gt;10&lt;/sub&gt; scale)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;As the histogram in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:length&quot;&gt;@fig:length&lt;/span&gt; shows, these go well beyond gusty descriptions like our &#8220;poached farmed Norwegian salmon&#8221;&#8212;the longest, from first class on a &lt;a href=&quot;http://menus.nypl.org/menus/29358&quot;&gt;1993 Virgin Atlantic flight&lt;/a&gt;, reads with the paragraph breaks removed like a deranged Basil Fawlty:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Afternoon Tea- A Great British Tradition- Tea, the most universally consumed of all drinks, is especially popular in Britain where the annual consumption is something in the region of 512 million cups. W. E. Gladstone observed &#8220;If you are cold, tea will warm you- if you are heated, it will cool you- if you are depressed, it will cheer you- if you are excited, it will calm you.&#8221; First brought to England c. 1559 by Giambattista Rusmusio, tea did not evolve into an afternoon meal until the end of the 18th century. Anna, Duchess of Bedford, invented afternoon tea to fill the long gap between early lunch and dinner which bored many house parties. It became a meal surrounded by etiquette and customs, delicate china, silver, cake stands and doilies- a time when friend and family meet. Famous tea parties include Mad Hatter&#8217;s (Alice&#8217;s Adventures in Wonderland by Lewis Carroll 1865), the Boston Tea Party, 1773, and not forgetting HM Queen Elizabeth II&#8217;s annual garden parties at Buckingham Palace. The Duke of Wellington declared that &#8220;Tea cleared my head and left no misapprehensions.&#8221; He was right- tea contains small amounts of two B vitamins, and has no calories, artificial flavourings or colourings. It is said to cure gout, apoplexy, epilepsy, gall stones and sleepiness, and one&#8217;s longevity is assured. &#8220;Thank God for Tea! What would the world do without tea?&#8221;- Sydney Smith&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Like the erroneous dates, these were sparse&#8212;only 6,682, or 0.5% of the listed dishes exceeded 100 characters, and these held only 3.7% of the dataset&#8217;s total characters. However, the decision to drop them was less clear-cut, since they potentially contained period-specific text which could be used to inform a model. I ultimately stetted them for this reason.&lt;/p&gt;
&lt;p&gt;I then processed the text. &lt;!-- were listed in foreign languages,  --&gt; 251 menu items contained non-ASCII characters, of which 126 were in French,&lt;a href=&quot;#fn3&quot; class=&quot;footnote-ref&quot; id=&quot;fnref3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; 56 Chinese, 21 German, 20 Swedish, 11 Greek, 10 Hindi, 3 Hungarian, and one lonely item in Polish. The remaining three were English with special characters such as &#189;. Happily, the excellent Python library &lt;a href=&quot;https://pypi.org/project/Unidecode/&quot;&gt;Unidecode&lt;/a&gt; can do most of the heavy lifting here, stripping accents, and Romanizing the Greek, Hindi, and Chinese.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb3&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; unidecode &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; unidecode&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-3&quot; title=&quot;3&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Remove special characters&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-4&quot; title=&quot;4&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].&lt;span class=&quot;bu&quot;&gt;apply&lt;/span&gt;(unidecode)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-5&quot; title=&quot;5&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-6&quot; title=&quot;6&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Check for remaining non-ASCII characters&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-7&quot; title=&quot;7&quot;&gt;df[df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].&lt;span class=&quot;bu&quot;&gt;str&lt;/span&gt;.match(&lt;span class=&quot;st&quot;&gt;&amp;quot;[^&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\x00&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\x7F&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;]&amp;quot;&lt;/span&gt;)][&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It then remained only to normalize to lower case and tokenize on the regular expression &lt;code&gt;[a-z&apos;-]+&lt;/code&gt;, which captures strings of letters, apostrophes, and hyphens and throws out other punctuation and numbers (since this destroys ordering information, I put it into a new column called &lt;code&gt;tokens&lt;/code&gt;).&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb4&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; nltk.tokenize &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; RegexpTokenizer&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-2&quot; title=&quot;2&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; nltk.corpus &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; stopwords&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-4&quot; title=&quot;4&quot;&gt;tokenizer &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; RegexpTokenizer(&lt;span class=&quot;st&quot;&gt;&amp;quot;[a-z&amp;#39;-]+&amp;quot;&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-5&quot; title=&quot;5&quot;&gt;stopwords &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;set&lt;/span&gt;(stopwords.words(&lt;span class=&quot;st&quot;&gt;&amp;quot;english&amp;quot;&lt;/span&gt;))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-6&quot; title=&quot;6&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-7&quot; title=&quot;7&quot;&gt;&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; tokenize(text):&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-8&quot; title=&quot;8&quot;&gt;    tokens &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;set&lt;/span&gt;(tokenizer.tokenize(text))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-9&quot; title=&quot;9&quot;&gt;    &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot; &amp;quot;&lt;/span&gt;.join(tokens &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt; stopwords)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-10&quot; title=&quot;10&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-11&quot; title=&quot;11&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].&lt;span class=&quot;bu&quot;&gt;str&lt;/span&gt;.lower()&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-12&quot; title=&quot;12&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].&lt;span class=&quot;bu&quot;&gt;apply&lt;/span&gt;(tokenize)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I also threw out &lt;a href=&quot;https://en.wikipedia.org/wiki/Stop_words&quot;&gt;stopwords&lt;/a&gt; such as &#8220;the&#8221;. The text was now ready for exploring trends and building a predictive model.&lt;/p&gt;
&lt;h1 id=&quot;data-exploration&quot;&gt;Data exploration&lt;/h1&gt;
&lt;h2 id=&quot;cross-sectional&quot;&gt;Cross-sectional&lt;/h2&gt;
&lt;p&gt;I first examined the top words from each decade, excerpted in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:1850&quot;&gt;@fig:1850&lt;/span&gt;&#8211;&lt;span class=&quot;citation&quot; data-cites=&quot;fig:1990&quot;&gt;@fig:1990&lt;/span&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-decade-1850.svg&quot; alt=&quot;Most common words on menus from the 1850s&quot; id=&quot;fig:1850&quot; /&gt;&lt;figcaption&gt;Most common words on menus from the 1850s&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;There are only a few menus from the 1850s in the collection, but we can get a sense of the palate. Names of wines&#8212;&lt;em&gt;madeira&lt;/em&gt;, &lt;em&gt;sherry&lt;/em&gt;, and &lt;em&gt;claret&lt;/em&gt;, and probably also &lt;em&gt;ch&#226;teau&lt;/em&gt; and &lt;em&gt;pale&lt;/em&gt;&#8212;feature prominently, and the most common cooking methods are boiling and roasting.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-decade-1910.svg&quot; alt=&quot;Most common words on menus from the 1910s&quot; id=&quot;fig:1910&quot; /&gt;&lt;figcaption&gt;Most common words on menus from the 1910s&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;By the 1910s, we have a new picture: fried foods are popular, as are cream sauces. Chicken and beef have made the list, as, notably, does salad, as fresh fruits and vegetables become more available to the average patron.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-decade-1950.svg&quot; alt=&quot;Most common words on menus from the 1950s&quot; id=&quot;fig:1950&quot; /&gt;&lt;figcaption&gt;Most common words on menus from the 1950s&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We see fewer changes in the post-war era, but &#8220;fresh&#8221; has been advanced, as have French &lt;em&gt;de&lt;/em&gt; &#8216;of&#8217; and German &lt;em&gt;mit&lt;/em&gt; &#8216;with&#8217;, indicating European dishes, or at least European phrasings, coming into vogue.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-decade-1990.svg&quot; alt=&quot;Most common words on menus from the 1990s&quot; id=&quot;fig:1990&quot; /&gt;&lt;figcaption&gt;Most common words on menus from the 1990s&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In the 1990s, &#8220;fried&#8221; has been replaced by &#8220;grilled&#8221;, and a renewed interest in French cuisine seems to be the &lt;em&gt;dernier cri&lt;/em&gt;, as five of the ten are French grammatical words not filtered by the English stopword list (and &lt;em&gt;de&lt;/em&gt; outpacing the next-highest word by almost 2 to 1).&lt;/p&gt;
&lt;h2 id=&quot;longitudinal&quot;&gt;Longitudinal&lt;/h2&gt;
&lt;p&gt;Equally illuminating is to look at the waxing and waning of particular foods across time, in the style of &lt;a href=&quot;https://books.google.com/ngrams&quot;&gt;Google Ngrams&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb5&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-1&quot; title=&quot;1&quot;&gt;words &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; [&lt;span class=&quot;st&quot;&gt;&amp;quot;lobster&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;oyster&amp;quot;&lt;/span&gt;]&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-3&quot; title=&quot;3&quot;&gt;occurrences &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; {word: df[&lt;span class=&quot;st&quot;&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;].&lt;span class=&quot;bu&quot;&gt;str&lt;/span&gt;.contains(&lt;span class=&quot;ss&quot;&gt;f&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;{&lt;/span&gt;word&lt;span class=&quot;sc&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;b&amp;quot;&lt;/span&gt;) &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; word &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; words}&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-4&quot; title=&quot;4&quot;&gt;menu_item_prop &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.DataFrame(occurrences).join([df[&lt;span class=&quot;st&quot;&gt;&amp;quot;year&amp;quot;&lt;/span&gt;]).groupby(&lt;span class=&quot;st&quot;&gt;&amp;quot;year&amp;quot;&lt;/span&gt;).mean()&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The Madeira that was so popular in the 1850s dropped off steeply soon after (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:madeira&quot;&gt;@fig:madeira&lt;/span&gt;).&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-term-madeira.svg&quot; alt=&quot;Percent of menu items per year containing the word &#8220;Madeira&#8221;&quot; id=&quot;fig:madeira&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing the word &#8220;Madeira&#8221;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We see the Sun rise and set on the age of Jell-O in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:jello&quot;&gt;@fig:jello&lt;/span&gt;, and in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:tofu&quot;&gt;@fig:tofu&lt;/span&gt; the nascence of tofu.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-term-jell-o.svg&quot; alt=&quot;Percent of menu items per year containing the word &#8220;Jell-O&#8221;&quot; id=&quot;fig:jello&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing the word &#8220;Jell-O&#8221;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-term-tofu.svg&quot; alt=&quot;Percent of menu items per year containing the word &#8220;tofu&#8221;&quot; id=&quot;fig:tofu&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing the word &#8220;tofu&#8221;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It is also interesting to look at descriptors. Organic food is rooted in the environmental movement of the &#8217;60s and &#8217;70s, but doesn&#8217;t appear on restaurant menus until the turn of the millennium.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-term-organic.svg&quot; alt=&quot;Percent of menu items per year containing the word &#8220;organic&#8221;&quot; id=&quot;fig:organic&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing the word &#8220;organic&#8221;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Health terms such as &#8220;diet&#8221; show a similar trend, appearing in numbers in the &#8217;70s.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-term-health-healthy-diet-lite.svg&quot; alt=&quot;Percent of menu items per year containing health terms  &#160;&#8220;health&#8221; &#160;&#8220;healthy&#8221; &#160;&#8220;diet&#8221; &#160;&#8220;lite&#8221; &quot; id=&quot;fig:health&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing health terms&lt;br /&gt;
&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#e7298a&quot; /&gt;&lt;/svg&gt;&#160;&#8220;health&#8221;&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#66a61e&quot; /&gt;&lt;/svg&gt;&#160;&#8220;healthy&#8221;&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#e6ab02&quot; /&gt;&lt;/svg&gt;&#160;&#8220;diet&#8221;&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#a6761d&quot; /&gt;&lt;/svg&gt;&#160;&#8220;lite&#8221;&lt;br /&gt;
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We can also use foreign words that commonly appear on menus as a rough proxy for how fashionable those cuisines were in different periods.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;menu-term-au-alla-mit.svg&quot; alt=&quot;Percent of menu items per year containing foreign prepositions  &#160;au (Fr. &#8216;in the style of&#8217;) &#160;alla (It. &#8216;in the style of&#8217;) &#160;mit (Gm. &#8216;with&#8217;)&quot; id=&quot;fig:auallamit&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing foreign prepositions&lt;br /&gt;
&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#e7298a&quot; /&gt;&lt;/svg&gt;&#160;&lt;em&gt;au&lt;/em&gt; (Fr. &#8216;in the style of&#8217;)&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#66a61e&quot; /&gt;&lt;/svg&gt;&#160;&lt;em&gt;alla&lt;/em&gt; (It. &#8216;in the style of&#8217;)&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#e6ab02&quot; /&gt;&lt;/svg&gt;&#160;&lt;em&gt;mit&lt;/em&gt; (Gm. &#8216;with&#8217;)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;A clear post-war interest in French and German cuisine manifests itself&#8212;although the 40% of menu items in 2005 containing &lt;em&gt;mit&lt;/em&gt; is more likely an artifact of a small sample than a genuine trend.&lt;/p&gt;
&lt;h1 id=&quot;modeling&quot;&gt;Modeling&lt;/h1&gt;
&lt;p&gt;The first step is to sample from the data, firstly to correct the class imbalance seen in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:decade&quot;&gt;@fig:decade&lt;/span&gt; above, but more importantly, to curtail the incredibly large matrix we would get if we vectorized the entire data frame.&lt;a href=&quot;#fn4&quot; class=&quot;footnote-ref&quot; id=&quot;fnref4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb6&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;co&quot;&gt;# We want each class to have only as many as the smallest class&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-2&quot; title=&quot;2&quot;&gt;sample_size &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df.groupby(&lt;span class=&quot;st&quot;&gt;&amp;quot;decade&amp;quot;&lt;/span&gt;)[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].count().&lt;span class=&quot;bu&quot;&gt;min&lt;/span&gt;()&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-4&quot; title=&quot;4&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Sample randomly from each decade&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-5&quot; title=&quot;5&quot;&gt;sample &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df.sample(frac&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;).groupby(&lt;span class=&quot;st&quot;&gt;&amp;quot;decade&amp;quot;&lt;/span&gt;).head(sample_size)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then we define our variables and create train and test classes:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb7&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; train_test_split&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-3&quot; title=&quot;3&quot;&gt;X &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; sample[&lt;span class=&quot;st&quot;&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;]&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-4&quot; title=&quot;4&quot;&gt;y &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; sample[&lt;span class=&quot;st&quot;&gt;&amp;quot;decade&amp;quot;&lt;/span&gt;]&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-5&quot; title=&quot;5&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-6&quot; title=&quot;6&quot;&gt;X_train, X_test, y_train, y_test &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; train_test_split(X, y, stratify&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;y)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since the &lt;code&gt;tokens&lt;/code&gt; column has already been cleaned, we can it for modeling using &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;tf-idf&lt;/a&gt; vectorization, which assigns high scores to words which are highly localized, occurring, say, only in the 1970s and nowhere else. This turns a vector of words into a vector of tf-idf scores.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb8&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.feature_extraction.text &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; TfidfVectorizer&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-3&quot; title=&quot;3&quot;&gt;tfidf &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; TfidfVectorizer()&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-4&quot; title=&quot;4&quot;&gt;X_train_vec &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; tfidf.fit_transform(X_train)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-5&quot; title=&quot;5&quot;&gt;X_test_vec &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; tfidf.transform(X_test)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, the model itself can be constructed. I use a Gaussian na&#239;ve Bayes classifier, which is standard despite the data not following a Gaussian distribution.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb9&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb9-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.naive_bayes &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; GaussianNB&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb9-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb9-3&quot; title=&quot;3&quot;&gt;bayes &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; GaussianNB().fit(X_train_vec.toarray(), y_train)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, we assess the accuracy of the model.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb10&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb10-1&quot; title=&quot;1&quot;&gt;bayes.score(X_train_vec.toarray(), y_train)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb10-2&quot; title=&quot;2&quot;&gt;bayes.score(X_test_vec.toarray(), y_test)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The results are pretty disappointing: 77.5% train accuracy and 17.6% test accuracy mean that the model arbitrarily latched onto some words that weren&#8217;t really indicative of their decades. This is due to our miserliness with the data: the model was only allowed to see 0.2% of the menu items. We can fix this by taking the second-lowest class rather than the lowest. This will give us &lt;em&gt;mostly&lt;/em&gt; balanced classes, but with fewer in the 1870s, to which only 174 menus are dated.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb11&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb11-1&quot; title=&quot;1&quot;&gt;sample_size &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df.groupby(&lt;span class=&quot;st&quot;&gt;&amp;quot;decade&amp;quot;&lt;/span&gt;)[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].count().sort_values().iloc[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The results are more promising: 52.7% train and 21.7% test accuracy. Another consideration is that, since our classes are ordinal, even when the model is wrong, it may only be wrong by a decade or two. We can check this by defining a &#8220;fuzzy accuracy&#8221; score, and seeing how&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb12&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb12-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; fuzzy_accuracy(y_true, y_pred, tolerance):&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb12-2&quot; title=&quot;2&quot;&gt;    &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; np.mean(np.&lt;span class=&quot;bu&quot;&gt;abs&lt;/span&gt;(y_true &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt; y_pred) &lt;span class=&quot;op&quot;&gt;&amp;lt;=&lt;/span&gt; tolerance)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb12-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb12-4&quot; title=&quot;4&quot;&gt;fuzzy_accuracy(y_train, bayes.predict(X_train_vec.toarray()), tolerance&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb12-5&quot; title=&quot;5&quot;&gt;fuzzy_accuracy(y_test, bayes.predict(X_test_vec.toarray()), tolerance&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We now get 62.7% train and 40.2% test accuracy with a tolerance of one decade. With more memory available, it would be possible to feed more of the dataset into the model, and perhaps create a yet more accurate model.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;In 2020, $7.85 and $25.29, respectively&lt;a href=&quot;#fnref1&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;Retrieved April 27, 2020&lt;a href=&quot;#fnref2&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;Or else in what Chesterton called &#8220;a sort of super-French employed by cooks, but quite unintelligible to Frenchmen&#8221;&lt;a href=&quot;#fnref3&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn4&quot;&gt;&lt;p&gt;Sampling function borrowed from &lt;a href=&quot;https://stackoverflow.com/a/56841648&quot; class=&quot;uri&quot;&gt;https://stackoverflow.com/a/56841648&lt;/a&gt;&lt;a href=&quot;#fnref4&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
	</description>
		</item>
		<item>
			<title>COVID-19 Sentiment Analysis</title>
			<link>https://alexklapheke.github.io/blog/publish/covid-sentiment.md</link>
			<pubDate>2020-06-23T15:58:12-04:00</pubDate>
	<description>
&lt;p&gt;This project is joint work with Luken Weaver, Jon Godin, and Reza Farrokhi.&lt;/p&gt;
&lt;div class=&quot;epigraph&quot;&gt;
&lt;p&gt;The Master said, &#8220;When the multitude hate a man, it is necessary to examine into the case. When the multitude like a man, it is necessary to examine into the case.&#8221;&lt;/p&gt;
&lt;/div&gt;
&lt;h1 id=&quot;background-problem-statement&quot;&gt;Background &amp;amp; problem statement&lt;/h1&gt;
&lt;p&gt;The COVID-19 crisis officially began in the US on January 20, when an infected traveler in Wuhan flew home to Washington State &lt;span class=&quot;citation&quot; data-cites=&quot;holshue2020first&quot;&gt;[@holshue2020first]&lt;/span&gt;. For the several months thence, in the absence of a unified national policy, states have engaged in a complicated fandango of business closures, mask orders, and quarantines for visitors. While it would take a much more sophisticated data analysis to gauge their effectiveness, we can look at how these policies were popularly received using social media sentiment as a proxy.&lt;/p&gt;
&lt;p&gt;We chose to compare sentiment between two cities in which the saga took quite different turns: New York, which suffered both an alarming outbreak and an austere lockdown (ongoing at time of writing), and Houston, which had a much easier time, with fewer infections than New York City had deaths, and a lockdown lasting barely a month (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:nychou&quot;&gt;@fig:nychou&lt;/span&gt;).&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;covid-nyc-hou.svg&quot; alt=&quot;Case and death counts in New York City (all five boroughs) and in Harris County, Texas (which contains Houston) as of June 22. Data courtesy of the New York Times.   &#160;Cases  &#160;Deaths  &#160;Lockdown (statewide)&quot; id=&quot;fig:nychou&quot; /&gt;&lt;figcaption&gt;Case and death counts in New York City (all five boroughs) and in Harris County, Texas (which contains Houston) as of June 22. Data courtesy of the &lt;a href=&quot;https://github.com/nytimes/covid-19-data&quot;&gt;&lt;em&gt;New York Times&lt;/em&gt;&lt;/a&gt;.&lt;br&gt;&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#081220&quot; /&gt;&lt;/svg&gt;&#160;Cases&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#CE2A07&quot; /&gt;&lt;/svg&gt;&#160;Deaths&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;10&quot;&gt;&lt;rect x=&quot;0&quot; y=&quot;0&quot; width=&quot;20&quot; height=&quot;10&quot; fill=&quot;#E0E0E0&quot; stroke=&quot;none&quot; /&gt;&lt;/svg&gt;&#160;Lockdown (statewide)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;A na&#239;ve hypothesis would be that Houstonians, faring better overall, would speak with more positive affect. Complicating this, of course, is the profusion of topics on which people might converse, including: China, Donald Trump, Anthony Fauci, the CDC and WHO, governors Andrew Cuomo&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and Greg Abbott&lt;a href=&quot;#fn2&quot; class=&quot;footnote-ref&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, cruise ships, quarantines, face masks, and the cancellations of events, not to mention the pathogen itself and its physiological effects. Our goal, therefore, was to isolate topics of conversation, and examine the sentiment surrounding particular public figures.&lt;/p&gt;
&lt;h1 id=&quot;data-collection&quot;&gt;Data collection&lt;/h1&gt;
&lt;p&gt;Our preferred data source was Twitter, which, with a third of a billion users, captures an enormous fraction of the public discourse. In addition, there is an up-to-date, curated &lt;a href=&quot;https://github.com/thepanacealab/covid19_twitter&quot;&gt;dataset&lt;/a&gt; of COVID-19 related tweets &lt;span class=&quot;citation&quot; data-cites=&quot;banda2020a&quot;&gt;[@banda2020a]&lt;/span&gt;. However, time and budget allowed few tweets to be captured&#8212;either on the order of 10&lt;sup&gt;3&lt;/sup&gt; total, or stretching back only seven days. In addition, the paucity of geotags&#8212;by &lt;a href=&quot;https://www.forbes.com/sites/kalevleetaru/2019/03/04/visualizing-seven-years-of-twitters-evolution-2012-2018/&quot;&gt;some estimates&lt;/a&gt;, on the order of 1%&#8212;would make filtering by city nearly impossible.&lt;/p&gt;
&lt;p&gt;Reddit turned out to be somewhat more hospitable, as &lt;a href=&quot;https://github.com/pushshift/api&quot;&gt;pushshift&lt;/a&gt; provides free access to the Reddit API, as well as a &lt;a href=&quot;http://files.pushshift.io/reddit/comments/&quot;&gt;full dataset&lt;/a&gt; of Reddit comments stretching back to the site&#8217;s founding &lt;span class=&quot;citation&quot; data-cites=&quot;baumgartner2020pushshift&quot;&gt;[@baumgartner2020pushshift]&lt;/span&gt;, although the latter source was not updated frequently enough for our purpose. Reddit users subscribe to interest communities called &#8220;subreddits&#8221;, prefixed with &#8220;/r/&#8221;, some of which are for cities and other locales. While it can&#8217;t be verified that a particular subscriber is a resident, we assume that a large proportion of them are. One downside of this structure is that subreddits can become &lt;a href=&quot;https://en.wikipedia.org/wiki/Echo_chamber_(media)&quot;&gt;echo chambers&lt;/a&gt; which do not represent the larger population.&lt;/p&gt;
&lt;!-- and gain (or lose) points for posting interesting, helpful, or funny comments in those communities. --&gt;
&lt;p&gt;Another difficulty is that although Reddit ranks in the &lt;a href=&quot;https://www.alexa.com/siteinfo/reddit.com&quot;&gt;top 20 websites&lt;/a&gt; by traffic, the volume of posts pales next to Twitter. Although each metropolitan subreddit records hundreds of thousands of subscribers, by the &lt;a href=&quot;https://en.wikipedia.org/wiki/1%25_rule_(Internet_culture)&quot;&gt;90&#8211;9&#8211;1 rule of thumb&lt;/a&gt;, we guessed that in a subreddit with on the order of 10&lt;sup&gt;5&lt;/sup&gt; subscribers, only about 10&lt;sup&gt;3&lt;/sup&gt; would be active commenters, as shown in &lt;span class=&quot;citation&quot; data-cites=&quot;tbl:commenters&quot;&gt;@tbl:commenters&lt;/span&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;City and subreddit populations and commenter estimates {#tbl:commenters}&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th&gt;Subreddit&lt;/th&gt;
&lt;th&gt;City pop.&lt;/th&gt;
&lt;th&gt;Subscribers&lt;/th&gt;
&lt;th&gt;Casual commenters&lt;/th&gt;
&lt;th&gt;Active commenters&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;/r/nyc&lt;/td&gt;
&lt;td&gt;8,300,000&lt;/td&gt;
&lt;td&gt;225,000&lt;/td&gt;
&lt;td&gt;&#8776;20,000&lt;/td&gt;
&lt;td&gt;&#8776;2,000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;/r/houston&lt;/td&gt;
&lt;td&gt;2,300,000&lt;/td&gt;
&lt;td&gt;150,000&lt;/td&gt;
&lt;td&gt;&#8776;13,500&lt;/td&gt;
&lt;td&gt;&#8776;1,500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We focused on comments, since most of the substantive (read: sentiment-laden) discussion happens there; however, these are not guaranteed to contain topic-relevant keywords, so we searched for posts whose titles contained any of the keywords &#8220;COVID&#8221;, &#8220;coronavirus&#8221;, &#8220;quarantine&#8221;, or &#8220;pandemic&#8221;. Reddit &lt;a href=&quot;https://github.com/reddit-archive/reddit/blob/master/r2/r2/lib/utils/_utils.pyx#L39-L40&quot;&gt;generates&lt;/a&gt; a base 36 ID for each post and comment on the site, so we collated post IDs with comment IDs, as the simplified code snippet below illustrates.&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb1&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; requests&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-2&quot; title=&quot;2&quot;&gt;&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; pd&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-4&quot; title=&quot;4&quot;&gt;comments &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; []&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-5&quot; title=&quot;5&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-6&quot; title=&quot;6&quot;&gt;&lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; subreddit &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; [&lt;span class=&quot;st&quot;&gt;&amp;quot;nyc&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;houston&amp;quot;&lt;/span&gt;]:&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-7&quot; title=&quot;7&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-8&quot; title=&quot;8&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Get posts with the most comments&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-9&quot; title=&quot;9&quot;&gt;    res &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; requests.get(&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-10&quot; title=&quot;10&quot;&gt;        &lt;span class=&quot;st&quot;&gt;&amp;quot;https://api.pushshift.io/reddit/search/submission&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-11&quot; title=&quot;11&quot;&gt;        params &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; {&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-12&quot; title=&quot;12&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;subreddit&amp;quot;&lt;/span&gt;: subreddit,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-13&quot; title=&quot;13&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;size&amp;quot;&lt;/span&gt;: &lt;span class=&quot;dv&quot;&gt;400&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-14&quot; title=&quot;14&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;sort_type&amp;quot;&lt;/span&gt;: &lt;span class=&quot;st&quot;&gt;&amp;quot;num_comments&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-15&quot; title=&quot;15&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;sort&amp;quot;&lt;/span&gt;: &lt;span class=&quot;st&quot;&gt;&amp;quot;desc&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-16&quot; title=&quot;16&quot;&gt;        })&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-17&quot; title=&quot;17&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-18&quot; title=&quot;18&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Parse JSON&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-19&quot; title=&quot;19&quot;&gt;    posts &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.DataFrame(res.json()[&lt;span class=&quot;st&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;])&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-20&quot; title=&quot;20&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-21&quot; title=&quot;21&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Get comments from posts&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-22&quot; title=&quot;22&quot;&gt;    res &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; requests.get(&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-23&quot; title=&quot;23&quot;&gt;        &lt;span class=&quot;st&quot;&gt;&amp;quot;https://api.pushshift.io/reddit/search/comment&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-24&quot; title=&quot;24&quot;&gt;        params &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; {&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-25&quot; title=&quot;25&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;subreddit&amp;quot;&lt;/span&gt;: subreddit,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-26&quot; title=&quot;26&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;link_id&amp;quot;&lt;/span&gt;: ([&lt;span class=&quot;st&quot;&gt;&amp;quot;t3_&amp;quot;&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt; n &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; n &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; posts[&lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;]]),&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-27&quot; title=&quot;27&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;size&amp;quot;&lt;/span&gt;: &lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-28&quot; title=&quot;28&quot;&gt;        })&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-29&quot; title=&quot;29&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-30&quot; title=&quot;30&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Collect parsed output&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-31&quot; title=&quot;31&quot;&gt;    comments.append(pd.DataFrame(res.json()[&lt;span class=&quot;st&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;]))&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In toto, we collected 150,000 comments from /r/nyc, and 85,000 from /r/houston, and randomly sampled 85,000 New York comments for the analysis to avoid variance issues.&lt;/p&gt;
&lt;h1 id=&quot;sentiment-analysis&quot;&gt;Sentiment analysis&lt;/h1&gt;
&lt;p&gt;The sentiment analysis itself was done with the VADER sentiment parser &lt;span class=&quot;citation&quot; data-cites=&quot;vader&quot;&gt;[@vader]&lt;/span&gt;, a sophisticated rule-based parser that incorporates contextual information such as negations (&#8220;not&#8221;), intensifiers (&#8220;very&#8221;), hedges (&#8220;somewhat&#8221;), and even emoticons (&#8220;:)&#8221;).&lt;a href=&quot;#fn3&quot; class=&quot;footnote-ref&quot; id=&quot;fnref3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; For these reasons, it works best on unprocessed text:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb2&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; nltk.sentiment.vader &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; SentimentIntensityAnalyzer&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-3&quot; title=&quot;3&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Instantiate sentiment analyzer &amp;amp; compute sentiments&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-4&quot; title=&quot;4&quot;&gt;analyzer &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; SentimentIntensityAnalyzer()&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-5&quot; title=&quot;5&quot;&gt;sentiments &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.DataFrame(&lt;span class=&quot;bu&quot;&gt;map&lt;/span&gt;(analyzer.polarity_scores, comments[&lt;span class=&quot;st&quot;&gt;&amp;quot;text&amp;quot;&lt;/span&gt;]))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-6&quot; title=&quot;6&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-7&quot; title=&quot;7&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Append to original data frame&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-8&quot; title=&quot;8&quot;&gt;comments &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; comments.join(sentiments.set_index(comments.index))&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It provides a &#8220;sentiment intensity&#8221; score which is signed for polarity (positive good, negative bad). We took the 5-day rolling mean of the sentiments of all comments, filtering for various keywords.&lt;/p&gt;
&lt;!-- ![](Sentiment-Governors-5day.svg){#fig:gov} --&gt;
&lt;p&gt;Two important caveats apply, the first of which is that knowing &lt;em&gt;what&lt;/em&gt; the sentiment is doesn&#8217;t tell you &lt;em&gt;why&lt;/em&gt;. For example, in late April, /r/nyc responded negatively to the keyword &#8220;Fauci&#8221;, but this turned out to be displeasure not with Fauci himself, but rather with Trump, who had considered firing him. Thus, sentiment &lt;em&gt;around&lt;/em&gt; a public figure should not be conflated with sentiment &lt;em&gt;toward&lt;/em&gt; that figure. The second caveat is that positive and negative sentiments can cancel; so although a highly positive or negative overall sentiment score reflects unanimity in these feelings, a neutral score does not represent apathy, but simply lack of consensus.&lt;/p&gt;
&lt;p&gt;One way consensus is achieved on Reddit is by voting: users vote for (&#8220;upvote&#8221;) or against (&#8220;downvote&#8221;) comments, and the comment&#8217;s score is the number of votes for minus the number against. If voting is a way of agreeing with the sentiment of a comment, then it can be seen as tacitly expressing the same sentiment. We accounted for this by eliminating comments that scored less than one, and weighting the sentiment of the remainder by their scores:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb3&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; np&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-3&quot; title=&quot;3&quot;&gt;comments[&lt;span class=&quot;st&quot;&gt;&amp;quot;weighted&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; np.maximum(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-4&quot; title=&quot;4&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Sentiment score&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-5&quot; title=&quot;5&quot;&gt;    comments[&lt;span class=&quot;st&quot;&gt;&amp;quot;compound&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-6&quot; title=&quot;6&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Reddit score&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-7&quot; title=&quot;7&quot;&gt;    comments[&lt;span class=&quot;st&quot;&gt;&amp;quot;score&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-8&quot; title=&quot;8&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Scaling factor (for convenience; does not change analysis)&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-9&quot; title=&quot;9&quot;&gt;    &lt;span class=&quot;fl&quot;&gt;0.75&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One potential pitfall is that since the highest-scoring comments appear highest on the page, this can be subject to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bandwagon_effect&quot;&gt;bandwagon effect&lt;/a&gt;, by which users see, and then upvote, comments that are already popular.&lt;/p&gt;
&lt;p&gt;We graphed the weighted sentiments of COVID-related posts in 2020 (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:2020&quot;&gt;@fig:2020&lt;/span&gt;), and formulated a baseline sentiment for comparison&#8212;maybe sentiment is more positive in haler times, or maybe New Yorkers really are &lt;a href=&quot;https://www.youtube.com/watch?v=lX4MoaTFp9g&quot;&gt;less positive&lt;/a&gt; in general&#8212;so we looked at comments from the same months in the previous year, without filtering by keyword (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:2019&quot;&gt;@fig:2019&lt;/span&gt;).&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;Sentiment-Citywide-2020-5day.svg&quot; alt=&quot;COVID-related sentiment across cities, 2020 (5-day rolling mean)   &#160;New York  &#160;Houston &quot; id=&quot;fig:2020&quot; /&gt;&lt;figcaption&gt;COVID-related sentiment across cities, 2020 (5-day rolling mean)&lt;br&gt;&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#FF5500&quot; /&gt;&lt;/svg&gt;&#160;New York&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#0658CF&quot; /&gt;&lt;/svg&gt;&#160;Houston&lt;br&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The overall results were unilluminating; the only particularly positive or negative periods, in February 2020, are essentially artifacts due to insufficient data (the virus had not yet seized the nation&#8217;s attention).&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;Sentiment-Citywide-2019-5day.svg&quot; alt=&quot;General sentiment across cities, 2019 (5-day rolling mean)   &#160;New York  &#160;Houston &quot; id=&quot;fig:2019&quot; /&gt;&lt;figcaption&gt;General sentiment across cities, 2019 (5-day rolling mean)&lt;br&gt;&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#FF5500&quot; /&gt;&lt;/svg&gt;&#160;New York&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#0658CF&quot; /&gt;&lt;/svg&gt;&#160;Houston&lt;br&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;A clearer picture arose around public figures. After filtering by the surname of each state&#8217;s governor, we see a positive spike in New York in the early days of shutdowns, whereas Houston takes a swift negative turn when Abbott declares lockdown, and swiftly becomes positive after he orders its end (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:gov&quot;&gt;@fig:gov&lt;/span&gt;).&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;Sentiment-Governors-5day.svg&quot; alt=&quot;Sentiment around governors across cities, 2020 (5-day rolling mean)   &#160;New York (Keyword &#8220;Cuomo&#8221;)  &#160;Houston (Keyword &#8220;Abbott&#8221;) &quot; id=&quot;fig:gov&quot; /&gt;&lt;figcaption&gt;Sentiment around governors across cities, 2020 (5-day rolling mean)&lt;br&gt;&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#FF5500&quot; /&gt;&lt;/svg&gt;&#160;New York (Keyword &#8220;Cuomo&#8221;)&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#0658CF&quot; /&gt;&lt;/svg&gt;&#160;Houston (Keyword &#8220;Abbott&#8221;)&lt;br&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This seems to imply that New Yorkers, having been much harder hit, were more sanguine about curtailments of public life than the relatively freewheeling Texans.&lt;/p&gt;
&lt;p&gt;A similarly revealing pattern is shown in response to the keyword &#8220;Trump&#8221; (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:trump&quot;&gt;@fig:trump&lt;/span&gt;). While comments about the president tended negative overall, they dipped steeply in late April, possibly in response to his defunding the WHO.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;Sentiment-Trump-5day.svg&quot; alt=&quot;Sentiment around President Trump across cities, 2020 (5-day rolling mean)   &#160;New York (Keyword &#8220;Trump&#8221;)  &#160;Houston (Keyword &#8220;Trump&#8221;) &quot; id=&quot;fig:trump&quot; /&gt;&lt;figcaption&gt;Sentiment around President Trump across cities, 2020 (5-day rolling mean)&lt;br&gt;&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#FF5500&quot; /&gt;&lt;/svg&gt;&#160;New York (Keyword &#8220;Trump&#8221;)&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#0658CF&quot; /&gt;&lt;/svg&gt;&#160;Houston (Keyword &#8220;Trump&#8221;)&lt;br&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;We have shown that public attitudes as expressed on social media can be tied to world events, and analyzed attitudes toward events during the COVID-19 crisis. This analysis could be expanded with, e.g., a more sophisticated weighting scheme, more precise filtering, or a larger dataset.&lt;/p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Dem., NY, in office since 2011&lt;a href=&quot;#fnref1&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;Rep., TX, in office since 2015&lt;a href=&quot;#fnref2&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;Some sample inputs and scores are shown below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th&gt;Text&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;great!&lt;/td&gt;
&lt;td&gt;+0.6588&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;great&lt;/td&gt;
&lt;td&gt;+0.6249&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;very good&lt;/td&gt;
&lt;td&gt;+0.4927&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;good&lt;/td&gt;
&lt;td&gt;+0.4404&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;not bad&lt;/td&gt;
&lt;td&gt;+0.4310&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;somewhat good&lt;/td&gt;
&lt;td&gt;+0.3832&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;okay&lt;/td&gt;
&lt;td&gt;+0.2263&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;meh&lt;/td&gt;
&lt;td&gt;&#8722;0.0772&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;not good&lt;/td&gt;
&lt;td&gt;&#8722;0.3412&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;bad&lt;/td&gt;
&lt;td&gt;&#8722;0.5423&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;a href=&quot;#fnref3&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
	</description>
		</item>
		<item>
			<title>Foreword</title>
			<link>https://alexklapheke.github.io/blog/publish/foreword.md</link>
			<pubDate>2020-06-22T06:10:59-04:00</pubDate>
	<description>
&lt;div class=&quot;epigraph&quot;&gt;
&lt;p&gt;I should not talk so much about myself if there were anybody else whom I knew as well.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As a preface to this blog, let me introduce myself: I&#8217;m a linguist turned data scientist living in the Boston area. While a lot of my work deals with natural language, I&#8217;m interested in the gamut of data and what knowledge can be gleaned from it. &lt;!-- Typifying this is the current COVID-19 crisis---Massachusetts is now in the thirteenth week of a state of emergency---during which uncertainty, epidemiological data (&quot;[The Curve](https://en.wikipedia.org/wiki/Epidemic_curve)&quot;) has been exalted almost to a kind of augury. --&gt; I won&#8217;t comment much on pop data science or world events, but I hope that through reasoned analysis I can provide clarity about what data I can get hold of.&lt;/p&gt;
&lt;p&gt;I entered data science after leaving academia, but the transition wasn&#8217;t abrupt. I&#8217;d learned to code some years prior, and ran experiments as a graduate student, punctiliously collecting small data sets, modeling them in R, and encapsulating the results in manuscripts and slides. The tools I use have changed (Python largely displacing R), and the data sets are some orders of magnitude larger, but my work in graduate school laid the flagstones for a data science career.&lt;/p&gt;
&lt;p&gt;This blog will mainly comprise overviews of the data projects I&#8217;m working on, essays on how I approach data, particularly language data, and posts about how I use various tools&#8212;not just programming languages and text editors, but also models and theorems. I hope not only to showcase what insights can be loosed from a dataset (and what illusory insights come of unsound analyses), but also serve as an aid and reference to other data scientists&#8212;not least among whom, my future self. A full portfolio of my work can be found on &lt;a href=&quot;https://alexklaphe.github.io/&quot;&gt;my homepage&lt;/a&gt;.&lt;/p&gt;
&lt;!-- The good news now is that, as the graph below shows, the curve seems to have peaked,[^rcode] and while it would be remiss to hazard that we&apos;re in the clear, or comment on the efficacy of this or that public health policy, I will say that I look forward to writing about my data investigations, and 

![New COVID-19 cases in Massachusetts (14-day moving average)](covid-mass.svg)\

[^rcode]: The full R code to generate this graph is below:

    ```r
    library(tidyverse)
    svg(&quot;covid-mass.svg&quot;, width = 5, height = 5)

    us_state &lt;- &quot;Massachusetts&quot;
    days &lt;- 14

    read.csv(&quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv&quot;) %&gt;%
    filter(state == us_state) %&gt;%
    mutate(date = as.POSIXct(date, format = &quot;%F&quot;)) %&gt;%
    mutate(newcases = cases - c(0, head(cases, -1))) %&gt;%
    mutate(newcases_moving = stats::filter(newcases, rep(1 / days, days))) %&gt;%
    ggplot(aes(x = date, y = newcases_moving)) +
    geom_point() +
    theme_classic() +
    theme(
            text = element_text(family = &quot;Linux Libertine O&quot;),
            axis.line = element_blank(),
            panel.grid.major.y = element_line(colour = &quot;gray90&quot;)
        ) +
    labs(
            title = paste0(&quot;New cases in &quot;, us_state, &quot; (&quot;, days, &quot;-day moving average)&quot;),
            subtitle = bquote(paste(&quot;Data courtesy of the &quot;, italic(&quot;New York Times&quot;),
                                    &quot;. Updated &quot;, .(format(Sys.time(), &quot;%b %-d, %Y&quot;)))),
            x = &quot;Date&quot;,
            y = &quot;New cases&quot;
        )
    ```
--&gt;
	</description>
		</item>
	</channel>
</rss>
