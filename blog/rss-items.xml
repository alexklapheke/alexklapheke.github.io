		</item>
	</description>
--&gt;
    ```
        )
            y = &quot;New cases&quot;
            x = &quot;Date&quot;,
                                    &quot;. Updated &quot;, .(format(Sys.time(), &quot;%b %-d, %Y&quot;)))),
            subtitle = bquote(paste(&quot;Data courtesy of the &quot;, italic(&quot;New York Times&quot;),
            title = paste0(&quot;New cases in &quot;, us_state, &quot; (&quot;, days, &quot;-day moving average)&quot;),
    labs(
        ) +
            panel.grid.major.y = element_line(colour = &quot;gray90&quot;)
            axis.line = element_blank(),
            text = element_text(family = &quot;Linux Libertine O&quot;),
    theme(
    theme_classic() +
    geom_point() +
    ggplot(aes(x = date, y = newcases_moving)) +
    mutate(newcases_moving = stats::filter(newcases, rep(1 / days, days))) %&gt;%
    mutate(newcases = cases - c(0, head(cases, -1))) %&gt;%
    mutate(date = as.POSIXct(date, format = &quot;%F&quot;)) %&gt;%
    filter(state == us_state) %&gt;%
    read.csv(&quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv&quot;) %&gt;%

    days &lt;- 14
    us_state &lt;- &quot;Massachusetts&quot;

    svg(&quot;covid-mass.svg&quot;, width = 5, height = 5)
    library(tidyverse)
    ```r

[^rcode]: The full R code to generate this graph is below:

![New COVID-19 cases in Massachusetts (14-day moving average)](covid-mass.svg)\

&lt;!-- The good news now is that, as the graph below shows, the curve seems to have peaked,[^rcode] and while it would be remiss to hazard that we&apos;re in the clear, or comment on the efficacy of this or that public health policy, I will say that I look forward to writing about my data investigations, and 
&lt;p&gt;This blog will mainly comprise overviews of the data projects I&#8217;m working on, essays on how I approach data, particularly language data, and posts about how I use various tools&#8212;not just programming languages and text editors, but also models and theorems. I hope not only to showcase what insights can be loosed from a dataset (and what illusory insights come of unsound analyses), but also serve as an aid and reference to other data scientists&#8212;not least among whom, my future self. A full portfolio of my work can be found on &lt;a href=&quot;https://alexklaphe.github.io/&quot;&gt;my homepage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I entered data science after leaving academia, but the transition wasn&#8217;t abrupt. I&#8217;d learned to code some years prior, and ran experiments as a graduate student, punctiliously collecting small data sets, modeling them in R, and encapsulating the results in manuscripts and slides. The tools I use have changed (Python largely displacing R), and the data sets are some orders of magnitude larger, but my work in graduate school laid the flagstones for a data science career.&lt;/p&gt;
&lt;p&gt;As a preface to this blog, let me introduce myself: I&#8217;m a linguist turned data scientist living in the Boston area. While a lot of my work deals with natural language, I&#8217;m interested in the gamut of data and what knowledge can be gleaned from it. &lt;!-- Typifying this is the current COVID-19 crisis---Massachusetts is now in the thirteenth week of a state of emergency---during which uncertainty, epidemiological data (&quot;[The Curve](https://en.wikipedia.org/wiki/Epidemic_curve)&quot;) has been exalted almost to a kind of augury. --&gt; I won&#8217;t comment much on pop data science or world events, but I hope that through reasoned analysis I can provide clarity about what data I can get hold of.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I should not talk so much about myself if there were anybody else whom I knew as well.&lt;/p&gt;
&lt;div class=&quot;epigraph&quot;&gt;
	<description>
			<pubDate>2020-06-22T06:10:59-04:00</pubDate>
			<link>https://alexklapheke.github.io/blog/publish/foreword.md</link>
			<title>Foreword</title>
		<item>
		</item>
	</description>
&lt;/section&gt;
&lt;/ol&gt;
&lt;a href=&quot;#fnref3&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/li&gt;
&lt;/table&gt;
&lt;/tbody&gt;
&lt;/tr&gt;
&lt;td&gt;&#8722;0.5423&lt;/td&gt;
&lt;td&gt;bad&lt;/td&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;/tr&gt;
&lt;td&gt;&#8722;0.3412&lt;/td&gt;
&lt;td&gt;not good&lt;/td&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;/tr&gt;
&lt;td&gt;&#8722;0.0772&lt;/td&gt;
&lt;td&gt;meh&lt;/td&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;/tr&gt;
&lt;td&gt;+0.2263&lt;/td&gt;
&lt;td&gt;okay&lt;/td&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;/tr&gt;
&lt;td&gt;+0.3832&lt;/td&gt;
&lt;td&gt;somewhat good&lt;/td&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;/tr&gt;
&lt;td&gt;+0.4310&lt;/td&gt;
&lt;td&gt;not bad&lt;/td&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;/tr&gt;
&lt;td&gt;+0.4404&lt;/td&gt;
&lt;td&gt;good&lt;/td&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;/tr&gt;
&lt;td&gt;+0.4927&lt;/td&gt;
&lt;td&gt;very good&lt;/td&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;/tr&gt;
&lt;td&gt;+0.6249&lt;/td&gt;
&lt;td&gt;great&lt;/td&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;/tr&gt;
&lt;td&gt;+0.6588&lt;/td&gt;
&lt;td&gt;great!&lt;/td&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;tbody&gt;
&lt;/thead&gt;
&lt;/tr&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;th&gt;Text&lt;/th&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;thead&gt;
&lt;table&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;Some sample inputs and scores are shown below:&lt;/p&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;Rep., TX, in office since 2015&lt;a href=&quot;#fnref2&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;Dem., NY, in office since 2011&lt;a href=&quot;#fnref1&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;ol&gt;
&lt;hr /&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;p&gt;We have shown that public attitudes as expressed on social media can be tied to world events, and analyzed attitudes toward events during the COVID-19 crisis. This analysis could be expanded with, e.g., a more sophisticated weighting scheme, more precise filtering, or a larger dataset.&lt;/p&gt;
&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;
&lt;/figure&gt;
&lt;img src=&quot;Sentiment-Trump-5day.svg&quot; alt=&quot;Sentiment around President Trump across cities, 2020 (5-day rolling mean)   &#160;New York (Keyword &#8220;Trump&#8221;)  &#160;Houston (Keyword &#8220;Trump&#8221;) &quot; id=&quot;fig:trump&quot; /&gt;&lt;figcaption&gt;Sentiment around President Trump across cities, 2020 (5-day rolling mean)&lt;br&gt;&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#FF5500&quot; /&gt;&lt;/svg&gt;&#160;New York (Keyword &#8220;Trump&#8221;)&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#0658CF&quot; /&gt;&lt;/svg&gt;&#160;Houston (Keyword &#8220;Trump&#8221;)&lt;br&gt;&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;A similarly revealing pattern is shown in response to the keyword &#8220;Trump&#8221; (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:trump&quot;&gt;@fig:trump&lt;/span&gt;). While comments about the president tended negative overall, they dipped steeply in late April, possibly in response to his defunding the WHO.&lt;/p&gt;
&lt;p&gt;This seems to imply that New Yorkers, having been much harder hit, were more sanguine about curtailments of public life than the relatively freewheeling Texans.&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;Sentiment-Governors-5day.svg&quot; alt=&quot;Sentiment around governors across cities, 2020 (5-day rolling mean)   &#160;New York (Keyword &#8220;Cuomo&#8221;)  &#160;Houston (Keyword &#8220;Abbott&#8221;) &quot; id=&quot;fig:gov&quot; /&gt;&lt;figcaption&gt;Sentiment around governors across cities, 2020 (5-day rolling mean)&lt;br&gt;&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#FF5500&quot; /&gt;&lt;/svg&gt;&#160;New York (Keyword &#8220;Cuomo&#8221;)&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#0658CF&quot; /&gt;&lt;/svg&gt;&#160;Houston (Keyword &#8220;Abbott&#8221;)&lt;br&gt;&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;A clearer picture arose around public figures. After filtering by the surname of each state&#8217;s governor, we see a positive spike in New York in the early days of shutdowns, whereas Houston takes a swift negative turn when Abbott declares lockdown, and swiftly becomes positive after he orders its end (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:gov&quot;&gt;@fig:gov&lt;/span&gt;).&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;Sentiment-Citywide-2019-5day.svg&quot; alt=&quot;General sentiment across cities, 2019 (5-day rolling mean)   &#160;New York  &#160;Houston &quot; id=&quot;fig:2019&quot; /&gt;&lt;figcaption&gt;General sentiment across cities, 2019 (5-day rolling mean)&lt;br&gt;&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#FF5500&quot; /&gt;&lt;/svg&gt;&#160;New York&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#0658CF&quot; /&gt;&lt;/svg&gt;&#160;Houston&lt;br&gt;&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;The overall results were unilluminating; the only particularly positive or negative periods, in February 2020, are essentially artifacts due to insufficient data (the virus had not yet seized the nation&#8217;s attention).&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;Sentiment-Citywide-2020-5day.svg&quot; alt=&quot;COVID-related sentiment across cities, 2020 (5-day rolling mean)   &#160;New York  &#160;Houston &quot; id=&quot;fig:2020&quot; /&gt;&lt;figcaption&gt;COVID-related sentiment across cities, 2020 (5-day rolling mean)&lt;br&gt;&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#FF5500&quot; /&gt;&lt;/svg&gt;&#160;New York&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#0658CF&quot; /&gt;&lt;/svg&gt;&#160;Houston&lt;br&gt;&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;We graphed the weighted sentiments of COVID-related posts in 2020 (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:2020&quot;&gt;@fig:2020&lt;/span&gt;), and formulated a baseline sentiment for comparison&#8212;maybe sentiment is more positive in haler times, or maybe New Yorkers really are &lt;a href=&quot;https://www.youtube.com/watch?v=lX4MoaTFp9g&quot;&gt;less positive&lt;/a&gt; in general&#8212;so we looked at comments from the same months in the previous year, without filtering by keyword (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:2019&quot;&gt;@fig:2019&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;One potential pitfall is that since the highest-scoring comments appear highest on the page, this can be subject to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bandwagon_effect&quot;&gt;bandwagon effect&lt;/a&gt;, by which users see, and then upvote, comments that are already popular.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-9&quot; title=&quot;9&quot;&gt;    &lt;span class=&quot;fl&quot;&gt;0.75&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-8&quot; title=&quot;8&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Scaling factor (for convenience; does not change analysis)&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-7&quot; title=&quot;7&quot;&gt;    comments[&lt;span class=&quot;st&quot;&gt;&amp;quot;score&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-6&quot; title=&quot;6&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Reddit score&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-5&quot; title=&quot;5&quot;&gt;    comments[&lt;span class=&quot;st&quot;&gt;&amp;quot;compound&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-4&quot; title=&quot;4&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Sentiment score&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-3&quot; title=&quot;3&quot;&gt;comments[&lt;span class=&quot;st&quot;&gt;&amp;quot;weighted&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; np.maximum(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb3&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; np&lt;/a&gt;
&lt;p&gt;One way consensus is achieved on Reddit is by voting: users vote for (&#8220;upvote&#8221;) or against (&#8220;downvote&#8221;) comments, and the comment&#8217;s score is the number of votes for minus the number against. If voting is a way of agreeing with the sentiment of a comment, then it can be seen as tacitly expressing the same sentiment. We accounted for this by eliminating comments that scored less than one, and weighting the sentiment of the remainder by their scores:&lt;/p&gt;
&lt;p&gt;Two important caveats apply, the first of which is that knowing &lt;em&gt;what&lt;/em&gt; the sentiment is doesn&#8217;t tell you &lt;em&gt;why&lt;/em&gt;. For example, in late April, /r/nyc responded negatively to the keyword &#8220;Fauci&#8221;, but this turned out to be displeasure not with Fauci himself, but rather with Trump, who had considered firing him. Thus, sentiment &lt;em&gt;around&lt;/em&gt; a public figure should not be conflated with sentiment &lt;em&gt;toward&lt;/em&gt; that figure. The second caveat is that positive and negative sentiments can cancel; so although a highly positive or negative overall sentiment score reflects unanimity in these feelings, a neutral score does not represent apathy, but simply lack of consensus.&lt;/p&gt;
&lt;!-- ![](Sentiment-Governors-5day.svg){#fig:gov} --&gt;
&lt;p&gt;It provides a &#8220;sentiment intensity&#8221; score which is signed for polarity (positive good, negative bad). We took the 5-day rolling mean of the sentiments of all comments, filtering for various keywords.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-8&quot; title=&quot;8&quot;&gt;comments &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; comments.join(sentiments.set_index(comments.index))&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-7&quot; title=&quot;7&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Append to original data frame&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-6&quot; title=&quot;6&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-5&quot; title=&quot;5&quot;&gt;sentiments &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.DataFrame(&lt;span class=&quot;bu&quot;&gt;map&lt;/span&gt;(analyzer.polarity_scores, comments[&lt;span class=&quot;st&quot;&gt;&amp;quot;text&amp;quot;&lt;/span&gt;]))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-4&quot; title=&quot;4&quot;&gt;analyzer &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; SentimentIntensityAnalyzer()&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-3&quot; title=&quot;3&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Instantiate sentiment analyzer &amp;amp; compute sentiments&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb2&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; nltk.sentiment.vader &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; SentimentIntensityAnalyzer&lt;/a&gt;
&lt;p&gt;The sentiment analysis itself was done with the VADER sentiment parser &lt;span class=&quot;citation&quot; data-cites=&quot;vader&quot;&gt;[@vader]&lt;/span&gt;, a sophisticated rule-based parser that incorporates contextual information such as negations (&#8220;not&#8221;), intensifiers (&#8220;very&#8221;), hedges (&#8220;somewhat&#8221;), and even emoticons (&#8220;:)&#8221;).&lt;a href=&quot;#fn3&quot; class=&quot;footnote-ref&quot; id=&quot;fnref3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; For these reasons, it works best on unprocessed text:&lt;/p&gt;
&lt;h1 id=&quot;sentiment-analysis&quot;&gt;Sentiment analysis&lt;/h1&gt;
&lt;p&gt;In toto, we collected 150,000 comments from /r/nyc, and 85,000 from /r/houston, and randomly sampled 85,000 New York comments for the analysis to avoid variance issues.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-31&quot; title=&quot;31&quot;&gt;    comments.append(pd.DataFrame(res.json()[&lt;span class=&quot;st&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;]))&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-30&quot; title=&quot;30&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Collect parsed output&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-29&quot; title=&quot;29&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-28&quot; title=&quot;28&quot;&gt;        })&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-27&quot; title=&quot;27&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;size&amp;quot;&lt;/span&gt;: &lt;span class=&quot;dv&quot;&gt;1000&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-26&quot; title=&quot;26&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;link_id&amp;quot;&lt;/span&gt;: ([&lt;span class=&quot;st&quot;&gt;&amp;quot;t3_&amp;quot;&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;+&lt;/span&gt; n &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; n &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; posts[&lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;]]),&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-25&quot; title=&quot;25&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;subreddit&amp;quot;&lt;/span&gt;: subreddit,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-24&quot; title=&quot;24&quot;&gt;        params &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; {&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-23&quot; title=&quot;23&quot;&gt;        &lt;span class=&quot;st&quot;&gt;&amp;quot;https://api.pushshift.io/reddit/search/comment&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-22&quot; title=&quot;22&quot;&gt;    res &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; requests.get(&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-21&quot; title=&quot;21&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Get comments from posts&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-20&quot; title=&quot;20&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-19&quot; title=&quot;19&quot;&gt;    posts &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.DataFrame(res.json()[&lt;span class=&quot;st&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;])&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-18&quot; title=&quot;18&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Parse JSON&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-17&quot; title=&quot;17&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-16&quot; title=&quot;16&quot;&gt;        })&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-15&quot; title=&quot;15&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;sort&amp;quot;&lt;/span&gt;: &lt;span class=&quot;st&quot;&gt;&amp;quot;desc&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-14&quot; title=&quot;14&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;sort_type&amp;quot;&lt;/span&gt;: &lt;span class=&quot;st&quot;&gt;&amp;quot;num_comments&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-13&quot; title=&quot;13&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;size&amp;quot;&lt;/span&gt;: &lt;span class=&quot;dv&quot;&gt;400&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-12&quot; title=&quot;12&quot;&gt;            &lt;span class=&quot;st&quot;&gt;&amp;quot;subreddit&amp;quot;&lt;/span&gt;: subreddit,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-11&quot; title=&quot;11&quot;&gt;        params &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; {&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-10&quot; title=&quot;10&quot;&gt;        &lt;span class=&quot;st&quot;&gt;&amp;quot;https://api.pushshift.io/reddit/search/submission&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-9&quot; title=&quot;9&quot;&gt;    res &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; requests.get(&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-8&quot; title=&quot;8&quot;&gt;    &lt;span class=&quot;co&quot;&gt;# Get posts with the most comments&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-7&quot; title=&quot;7&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-6&quot; title=&quot;6&quot;&gt;&lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; subreddit &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; [&lt;span class=&quot;st&quot;&gt;&amp;quot;nyc&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;houston&amp;quot;&lt;/span&gt;]:&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-5&quot; title=&quot;5&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-4&quot; title=&quot;4&quot;&gt;comments &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; []&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-2&quot; title=&quot;2&quot;&gt;&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;im&quot;&gt;as&lt;/span&gt; pd&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb1&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; requests&lt;/a&gt;
&lt;p&gt;We focused on comments, since most of the substantive (read: sentiment-laden) discussion happens there; however, these are not guaranteed to contain topic-relevant keywords, so we searched for posts whose titles contained any of the keywords &#8220;COVID&#8221;, &#8220;coronavirus&#8221;, &#8220;quarantine&#8221;, or &#8220;pandemic&#8221;. Reddit &lt;a href=&quot;https://github.com/reddit-archive/reddit/blob/master/r2/r2/lib/utils/_utils.pyx#L39-L40&quot;&gt;generates&lt;/a&gt; a base 36 ID for each post and comment on the site, so we collated post IDs with comment IDs, as the simplified code snippet below illustrates.&lt;/p&gt;
&lt;/table&gt;
&lt;/tbody&gt;
&lt;/tr&gt;
&lt;td&gt;&#8776;1,500&lt;/td&gt;
&lt;td&gt;&#8776;13,500&lt;/td&gt;
&lt;td&gt;150,000&lt;/td&gt;
&lt;td&gt;2,300,000&lt;/td&gt;
&lt;td&gt;/r/houston&lt;/td&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;/tr&gt;
&lt;td&gt;&#8776;2,000&lt;/td&gt;
&lt;td&gt;&#8776;20,000&lt;/td&gt;
&lt;td&gt;225,000&lt;/td&gt;
&lt;td&gt;8,300,000&lt;/td&gt;
&lt;td&gt;/r/nyc&lt;/td&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;tbody&gt;
&lt;/thead&gt;
&lt;/tr&gt;
&lt;th&gt;Active commenters&lt;/th&gt;
&lt;th&gt;Casual commenters&lt;/th&gt;
&lt;th&gt;Subscribers&lt;/th&gt;
&lt;th&gt;City pop.&lt;/th&gt;
&lt;th&gt;Subreddit&lt;/th&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;thead&gt;
&lt;caption&gt;City and subreddit populations and commenter estimates {#tbl:commenters}&lt;/caption&gt;
&lt;table&gt;
&lt;p&gt;Another difficulty is that although Reddit ranks in the &lt;a href=&quot;https://www.alexa.com/siteinfo/reddit.com&quot;&gt;top 20 websites&lt;/a&gt; by traffic, the volume of posts pales next to Twitter. Although each metropolitan subreddit records hundreds of thousands of subscribers, by the &lt;a href=&quot;https://en.wikipedia.org/wiki/1%25_rule_(Internet_culture)&quot;&gt;90&#8211;9&#8211;1 rule of thumb&lt;/a&gt;, we guessed that in a subreddit with on the order of 10&lt;sup&gt;5&lt;/sup&gt; subscribers, only about 10&lt;sup&gt;3&lt;/sup&gt; would be active commenters, as shown in &lt;span class=&quot;citation&quot; data-cites=&quot;tbl:commenters&quot;&gt;@tbl:commenters&lt;/span&gt;.&lt;/p&gt;
&lt;!-- and gain (or lose) points for posting interesting, helpful, or funny comments in those communities. --&gt;
&lt;p&gt;Reddit turned out to be somewhat more hospitable, as &lt;a href=&quot;https://github.com/pushshift/api&quot;&gt;pushshift&lt;/a&gt; provides free access to the Reddit API, as well as a &lt;a href=&quot;http://files.pushshift.io/reddit/comments/&quot;&gt;full dataset&lt;/a&gt; of Reddit comments stretching back to the site&#8217;s founding &lt;span class=&quot;citation&quot; data-cites=&quot;baumgartner2020pushshift&quot;&gt;[@baumgartner2020pushshift]&lt;/span&gt;, although the latter source was not updated frequently enough for our purpose. Reddit users subscribe to interest communities called &#8220;subreddits&#8221;, prefixed with &#8220;/r/&#8221;, some of which are for cities and other locales. While it can&#8217;t be verified that a particular subscriber is a resident, we assume that a large proportion of them are. One downside of this structure is that subreddits can become &lt;a href=&quot;https://en.wikipedia.org/wiki/Echo_chamber_(media)&quot;&gt;echo chambers&lt;/a&gt; which do not represent the larger population.&lt;/p&gt;
&lt;p&gt;Our preferred data source was Twitter, which, with a third of a billion users, captures an enormous fraction of the public discourse. In addition, there is an up-to-date, curated &lt;a href=&quot;https://github.com/thepanacealab/covid19_twitter&quot;&gt;dataset&lt;/a&gt; of COVID-19 related tweets &lt;span class=&quot;citation&quot; data-cites=&quot;banda2020a&quot;&gt;[@banda2020a]&lt;/span&gt;. However, time and budget allowed few tweets to be captured&#8212;either on the order of 10&lt;sup&gt;3&lt;/sup&gt; total, or stretching back only seven days. In addition, the paucity of geotags&#8212;by &lt;a href=&quot;https://www.forbes.com/sites/kalevleetaru/2019/03/04/visualizing-seven-years-of-twitters-evolution-2012-2018/&quot;&gt;some estimates&lt;/a&gt;, on the order of 1%&#8212;would make filtering by city nearly impossible.&lt;/p&gt;
&lt;h1 id=&quot;data-collection&quot;&gt;Data collection&lt;/h1&gt;
&lt;p&gt;A na&#239;ve hypothesis would be that Houstonians, faring better overall, would speak with more positive affect. Complicating this, of course, is the profusion of topics on which people might converse, including: China, Donald Trump, Anthony Fauci, the CDC and WHO, governors Andrew Cuomo&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and Greg Abbott&lt;a href=&quot;#fn2&quot; class=&quot;footnote-ref&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, cruise ships, quarantines, face masks, and the cancellations of events, not to mention the pathogen itself and its physiological effects. Our goal, therefore, was to isolate topics of conversation, and examine the sentiment surrounding particular public figures.&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;covid-nyc-hou.svg&quot; alt=&quot;Case and death counts in New York City (all five boroughs) and in Harris County, Texas (which contains Houston) as of June 22. Data courtesy of the New York Times.   &#160;Cases  &#160;Deaths  &#160;Lockdown (statewide)&quot; id=&quot;fig:nychou&quot; /&gt;&lt;figcaption&gt;Case and death counts in New York City (all five boroughs) and in Harris County, Texas (which contains Houston) as of June 22. Data courtesy of the &lt;a href=&quot;https://github.com/nytimes/covid-19-data&quot;&gt;&lt;em&gt;New York Times&lt;/em&gt;&lt;/a&gt;.&lt;br&gt;&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#081220&quot; /&gt;&lt;/svg&gt;&#160;Cases&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#CE2A07&quot; /&gt;&lt;/svg&gt;&#160;Deaths&lt;br&gt; &lt;svg width=&quot;20&quot; height=&quot;10&quot;&gt;&lt;rect x=&quot;0&quot; y=&quot;0&quot; width=&quot;20&quot; height=&quot;10&quot; fill=&quot;#E0E0E0&quot; stroke=&quot;none&quot; /&gt;&lt;/svg&gt;&#160;Lockdown (statewide)&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;We chose to compare sentiment between two cities in which the saga took quite different turns: New York, which suffered both an alarming outbreak and an austere lockdown (ongoing at time of writing), and Houston, which had a much easier time, with fewer infections than New York City had deaths, and a lockdown lasting barely a month (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:nychou&quot;&gt;@fig:nychou&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The COVID-19 crisis officially began in the US on January 20, when an infected traveler in Wuhan flew home to Washington State &lt;span class=&quot;citation&quot; data-cites=&quot;holshue2020first&quot;&gt;[@holshue2020first]&lt;/span&gt;. For the several months thence, in the absence of a unified national policy, states have engaged in a complicated fandango of business closures, mask orders, and quarantines for visitors. While it would take a much more sophisticated data analysis to gauge their effectiveness, we can look at how these policies were popularly received using social media sentiment as a proxy.&lt;/p&gt;
&lt;h1 id=&quot;background-problem-statement&quot;&gt;Background &amp;amp; problem statement&lt;/h1&gt;
&lt;/div&gt;
&lt;p&gt;The Master said, &#8220;When the multitude hate a man, it is necessary to examine into the case. When the multitude like a man, it is necessary to examine into the case.&#8221;&lt;/p&gt;
&lt;div class=&quot;epigraph&quot;&gt;
&lt;p&gt;This project is joint work with Luken Weaver, Jon Godin, and Reza Farrokhi.&lt;/p&gt;
	<description>
			<pubDate>2020-06-23T15:58:12-04:00</pubDate>
			<link>https://alexklapheke.github.io/blog/publish/covid-sentiment.md</link>
			<title>COVID-19 Sentiment Analysis</title>
		<item>
		</item>
	</description>
&lt;/section&gt;
&lt;/ol&gt;
&lt;li id=&quot;fn4&quot;&gt;&lt;p&gt;Sampling function borrowed from &lt;a href=&quot;https://stackoverflow.com/a/56841648&quot; class=&quot;uri&quot;&gt;https://stackoverflow.com/a/56841648&lt;/a&gt;&lt;a href=&quot;#fnref4&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;Or else in what Chesterton called &#8220;a sort of super-French employed by cooks, but quite unintelligible to Frenchmen&#8221;&lt;a href=&quot;#fnref3&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;Retrieved April 27, 2020&lt;a href=&quot;#fnref2&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;In 2020, $7.85 and $25.29, respectively&lt;a href=&quot;#fnref1&quot; class=&quot;footnote-back&quot;&gt;&#8617;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;ol&gt;
&lt;hr /&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;p&gt;We now get 62.7% train and 40.2% test accuracy with a tolerance of one decade. With more memory available, it would be possible to feed more of the dataset into the model, and perhaps create a yet more accurate model.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb12-5&quot; title=&quot;5&quot;&gt;fuzzy_accuracy(y_test, bayes.predict(X_test_vec.toarray()), tolerance&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb12-4&quot; title=&quot;4&quot;&gt;fuzzy_accuracy(y_train, bayes.predict(X_train_vec.toarray()), tolerance&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb12-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb12-2&quot; title=&quot;2&quot;&gt;    &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; np.mean(np.&lt;span class=&quot;bu&quot;&gt;abs&lt;/span&gt;(y_true &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt; y_pred) &lt;span class=&quot;op&quot;&gt;&amp;lt;=&lt;/span&gt; tolerance)&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb12&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb12-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; fuzzy_accuracy(y_true, y_pred, tolerance):&lt;/a&gt;
&lt;p&gt;The results are more promising: 52.7% train and 21.7% test accuracy. Another consideration is that, since our classes are ordinal, even when the model is wrong, it may only be wrong by a decade or two. We can check this by defining a &#8220;fuzzy accuracy&#8221; score, and seeing how&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb11&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb11-1&quot; title=&quot;1&quot;&gt;sample_size &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df.groupby(&lt;span class=&quot;st&quot;&gt;&amp;quot;decade&amp;quot;&lt;/span&gt;)[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].count().sort_values().iloc[&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The results are pretty disappointing: 77.5% train accuracy and 17.6% test accuracy mean that the model arbitrarily latched onto some words that weren&#8217;t really indicative of their decades. This is due to our miserliness with the data: the model was only allowed to see 0.2% of the menu items. We can fix this by taking the second-lowest class rather than the lowest. This will give us &lt;em&gt;mostly&lt;/em&gt; balanced classes, but with fewer in the 1870s, to which only 174 menus are dated.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb10-2&quot; title=&quot;2&quot;&gt;bayes.score(X_test_vec.toarray(), y_test)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb10&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb10-1&quot; title=&quot;1&quot;&gt;bayes.score(X_train_vec.toarray(), y_train)&lt;/a&gt;
&lt;p&gt;Finally, we assess the accuracy of the model.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb9-3&quot; title=&quot;3&quot;&gt;bayes &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; GaussianNB().fit(X_train_vec.toarray(), y_train)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb9-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb9&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb9-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.naive_bayes &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; GaussianNB&lt;/a&gt;
&lt;p&gt;Finally, the model itself can be constructed. I use a Gaussian na&#239;ve Bayes classifier, which is standard despite the data not following a Gaussian distribution.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-5&quot; title=&quot;5&quot;&gt;X_test_vec &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; tfidf.transform(X_test)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-4&quot; title=&quot;4&quot;&gt;X_train_vec &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; tfidf.fit_transform(X_train)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-3&quot; title=&quot;3&quot;&gt;tfidf &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; TfidfVectorizer()&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb8&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb8-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.feature_extraction.text &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; TfidfVectorizer&lt;/a&gt;
&lt;p&gt;Since the &lt;code&gt;tokens&lt;/code&gt; column has already been cleaned, we can it for modeling using &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;tf-idf&lt;/a&gt; vectorization, which assigns high scores to words which are highly localized, occurring, say, only in the 1970s and nowhere else. This turns a vector of words into a vector of tf-idf scores.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-6&quot; title=&quot;6&quot;&gt;X_train, X_test, y_train, y_test &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; train_test_split(X, y, stratify&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;y)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-5&quot; title=&quot;5&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-4&quot; title=&quot;4&quot;&gt;y &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; sample[&lt;span class=&quot;st&quot;&gt;&amp;quot;decade&amp;quot;&lt;/span&gt;]&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-3&quot; title=&quot;3&quot;&gt;X &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; sample[&lt;span class=&quot;st&quot;&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;]&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb7&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb7-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; train_test_split&lt;/a&gt;
&lt;p&gt;Then we define our variables and create train and test classes:&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-5&quot; title=&quot;5&quot;&gt;sample &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df.sample(frac&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;).groupby(&lt;span class=&quot;st&quot;&gt;&amp;quot;decade&amp;quot;&lt;/span&gt;).head(sample_size)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-4&quot; title=&quot;4&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Sample randomly from each decade&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-2&quot; title=&quot;2&quot;&gt;sample_size &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df.groupby(&lt;span class=&quot;st&quot;&gt;&amp;quot;decade&amp;quot;&lt;/span&gt;)[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].count().&lt;span class=&quot;bu&quot;&gt;min&lt;/span&gt;()&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb6&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb6-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;co&quot;&gt;# We want each class to have only as many as the smallest class&lt;/span&gt;&lt;/a&gt;
&lt;p&gt;The first step is to sample from the data, firstly to correct the class imbalance seen in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:decade&quot;&gt;@fig:decade&lt;/span&gt; above, but more importantly, to curtail the incredibly large matrix we would get if we vectorized the entire data frame.&lt;a href=&quot;#fn4&quot; class=&quot;footnote-ref&quot; id=&quot;fnref4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;modeling&quot;&gt;Modeling&lt;/h1&gt;
&lt;p&gt;A clear post-war interest in French and German cuisine manifests itself&#8212;although the 40% of menu items in 2005 containing &lt;em&gt;mit&lt;/em&gt; is more likely an artifact of a small sample than a genuine trend.&lt;/p&gt;
&lt;/figure&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#e6ab02&quot; /&gt;&lt;/svg&gt;&#160;&lt;em&gt;mit&lt;/em&gt; (Gm. &#8216;with&#8217;)&lt;/figcaption&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#66a61e&quot; /&gt;&lt;/svg&gt;&#160;&lt;em&gt;alla&lt;/em&gt; (It. &#8216;in the style of&#8217;)&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#e7298a&quot; /&gt;&lt;/svg&gt;&#160;&lt;em&gt;au&lt;/em&gt; (Fr. &#8216;in the style of&#8217;)&lt;br /&gt;
&lt;br /&gt;
&lt;img src=&quot;menu-term-au-alla-mit.svg&quot; alt=&quot;Percent of menu items per year containing foreign prepositions  &#160;au (Fr. &#8216;in the style of&#8217;) &#160;alla (It. &#8216;in the style of&#8217;) &#160;mit (Gm. &#8216;with&#8217;)&quot; id=&quot;fig:auallamit&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing foreign prepositions&lt;br /&gt;
&lt;figure&gt;
&lt;p&gt;We can also use foreign words that commonly appear on menus as a rough proxy for how fashionable those cuisines were in different periods.&lt;/p&gt;
&lt;/figure&gt;
&lt;/figcaption&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#a6761d&quot; /&gt;&lt;/svg&gt;&#160;&#8220;lite&#8221;&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#e6ab02&quot; /&gt;&lt;/svg&gt;&#160;&#8220;diet&#8221;&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#66a61e&quot; /&gt;&lt;/svg&gt;&#160;&#8220;healthy&#8221;&lt;br /&gt;
&lt;svg width=&quot;20&quot; height=&quot;4&quot;&gt;&lt;line x1=&quot;0&quot; y1=&quot;0&quot; x2=&quot;20&quot; y2=&quot;0&quot; stroke-width=&quot;4&quot; stroke=&quot;#e7298a&quot; /&gt;&lt;/svg&gt;&#160;&#8220;health&#8221;&lt;br /&gt;
&lt;br /&gt;
&lt;img src=&quot;menu-term-health-healthy-diet-lite.svg&quot; alt=&quot;Percent of menu items per year containing health terms  &#160;&#8220;health&#8221; &#160;&#8220;healthy&#8221; &#160;&#8220;diet&#8221; &#160;&#8220;lite&#8221; &quot; id=&quot;fig:health&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing health terms&lt;br /&gt;
&lt;figure&gt;
&lt;p&gt;Health terms such as &#8220;diet&#8221; show a similar trend, appearing in numbers in the &#8217;70s.&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-term-organic.svg&quot; alt=&quot;Percent of menu items per year containing the word &#8220;organic&#8221;&quot; id=&quot;fig:organic&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing the word &#8220;organic&#8221;&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;It is also interesting to look at descriptors. Organic food is rooted in the environmental movement of the &#8217;60s and &#8217;70s, but doesn&#8217;t appear on restaurant menus until the turn of the millennium.&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-term-tofu.svg&quot; alt=&quot;Percent of menu items per year containing the word &#8220;tofu&#8221;&quot; id=&quot;fig:tofu&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing the word &#8220;tofu&#8221;&lt;/figcaption&gt;
&lt;figure&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-term-jell-o.svg&quot; alt=&quot;Percent of menu items per year containing the word &#8220;Jell-O&#8221;&quot; id=&quot;fig:jello&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing the word &#8220;Jell-O&#8221;&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;We see the Sun rise and set on the age of Jell-O in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:jello&quot;&gt;@fig:jello&lt;/span&gt;, and in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:tofu&quot;&gt;@fig:tofu&lt;/span&gt; the nascence of tofu.&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-term-madeira.svg&quot; alt=&quot;Percent of menu items per year containing the word &#8220;Madeira&#8221;&quot; id=&quot;fig:madeira&quot; /&gt;&lt;figcaption&gt;Percent of menu items per year containing the word &#8220;Madeira&#8221;&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;The Madeira that was so popular in the 1850s dropped off steeply soon after (&lt;span class=&quot;citation&quot; data-cites=&quot;fig:madeira&quot;&gt;@fig:madeira&lt;/span&gt;).&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-4&quot; title=&quot;4&quot;&gt;menu_item_prop &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.DataFrame(occurrences).join([df[&lt;span class=&quot;st&quot;&gt;&amp;quot;year&amp;quot;&lt;/span&gt;]).groupby(&lt;span class=&quot;st&quot;&gt;&amp;quot;year&amp;quot;&lt;/span&gt;).mean()&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-3&quot; title=&quot;3&quot;&gt;occurrences &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; {word: df[&lt;span class=&quot;st&quot;&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;].&lt;span class=&quot;bu&quot;&gt;str&lt;/span&gt;.contains(&lt;span class=&quot;ss&quot;&gt;f&amp;quot;&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;sc&quot;&gt;{&lt;/span&gt;word&lt;span class=&quot;sc&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;b&amp;quot;&lt;/span&gt;) &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; word &lt;span class=&quot;kw&quot;&gt;in&lt;/span&gt; words}&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb5&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb5-1&quot; title=&quot;1&quot;&gt;words &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; [&lt;span class=&quot;st&quot;&gt;&amp;quot;lobster&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;oyster&amp;quot;&lt;/span&gt;]&lt;/a&gt;
&lt;p&gt;Equally illuminating is to look at the waxing and waning of particular foods across time, in the style of &lt;a href=&quot;https://books.google.com/ngrams&quot;&gt;Google Ngrams&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;longitudinal&quot;&gt;Longitudinal&lt;/h2&gt;
&lt;p&gt;In the 1990s, &#8220;fried&#8221; has been replaced by &#8220;grilled&#8221;, and a renewed interest in French cuisine seems to be the &lt;em&gt;dernier cri&lt;/em&gt;, as five of the ten are French grammatical words not filtered by the English stopword list (and &lt;em&gt;de&lt;/em&gt; outpacing the next-highest word by almost 2 to 1).&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-decade-1990.svg&quot; alt=&quot;Most common words on menus from the 1990s&quot; id=&quot;fig:1990&quot; /&gt;&lt;figcaption&gt;Most common words on menus from the 1990s&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;We see fewer changes in the post-war era, but &#8220;fresh&#8221; has been advanced, as have French &lt;em&gt;de&lt;/em&gt; &#8216;of&#8217; and German &lt;em&gt;mit&lt;/em&gt; &#8216;with&#8217;, indicating European dishes, or at least European phrasings, coming into vogue.&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-decade-1950.svg&quot; alt=&quot;Most common words on menus from the 1950s&quot; id=&quot;fig:1950&quot; /&gt;&lt;figcaption&gt;Most common words on menus from the 1950s&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;By the 1910s, we have a new picture: fried foods are popular, as are cream sauces. Chicken and beef have made the list, as, notably, does salad, as fresh fruits and vegetables become more available to the average patron.&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-decade-1910.svg&quot; alt=&quot;Most common words on menus from the 1910s&quot; id=&quot;fig:1910&quot; /&gt;&lt;figcaption&gt;Most common words on menus from the 1910s&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;There are only a few menus from the 1850s in the collection, but we can get a sense of the palate. Names of wines&#8212;&lt;em&gt;madeira&lt;/em&gt;, &lt;em&gt;sherry&lt;/em&gt;, and &lt;em&gt;claret&lt;/em&gt;, and probably also &lt;em&gt;ch&#226;teau&lt;/em&gt; and &lt;em&gt;pale&lt;/em&gt;&#8212;feature prominently, and the most common cooking methods are boiling and roasting.&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-decade-1850.svg&quot; alt=&quot;Most common words on menus from the 1850s&quot; id=&quot;fig:1850&quot; /&gt;&lt;figcaption&gt;Most common words on menus from the 1850s&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;I first examined the top words from each decade, excerpted in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:1850&quot;&gt;@fig:1850&lt;/span&gt;&#8211;&lt;span class=&quot;citation&quot; data-cites=&quot;fig:1990&quot;&gt;@fig:1990&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;cross-sectional&quot;&gt;Cross-sectional&lt;/h2&gt;
&lt;h1 id=&quot;data-exploration&quot;&gt;Data exploration&lt;/h1&gt;
&lt;p&gt;I also threw out &lt;a href=&quot;https://en.wikipedia.org/wiki/Stop_words&quot;&gt;stopwords&lt;/a&gt; such as &#8220;the&#8221;. The text was now ready for exploring trends and building a predictive model.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-12&quot; title=&quot;12&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].&lt;span class=&quot;bu&quot;&gt;apply&lt;/span&gt;(tokenize)&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-11&quot; title=&quot;11&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].&lt;span class=&quot;bu&quot;&gt;str&lt;/span&gt;.lower()&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-10&quot; title=&quot;10&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-9&quot; title=&quot;9&quot;&gt;    &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot; &amp;quot;&lt;/span&gt;.join(tokens &lt;span class=&quot;op&quot;&gt;-&lt;/span&gt; stopwords)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-8&quot; title=&quot;8&quot;&gt;    tokens &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;set&lt;/span&gt;(tokenizer.tokenize(text))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-7&quot; title=&quot;7&quot;&gt;&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; tokenize(text):&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-6&quot; title=&quot;6&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-5&quot; title=&quot;5&quot;&gt;stopwords &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;set&lt;/span&gt;(stopwords.words(&lt;span class=&quot;st&quot;&gt;&amp;quot;english&amp;quot;&lt;/span&gt;))&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-4&quot; title=&quot;4&quot;&gt;tokenizer &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; RegexpTokenizer(&lt;span class=&quot;st&quot;&gt;&amp;quot;[a-z&amp;#39;-]+&amp;quot;&lt;/span&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-2&quot; title=&quot;2&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; nltk.corpus &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; stopwords&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb4&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb4-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; nltk.tokenize &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; RegexpTokenizer&lt;/a&gt;
&lt;p&gt;It then remained only to normalize to lower case and tokenize on the regular expression &lt;code&gt;[a-z&apos;-]+&lt;/code&gt;, which captures strings of letters, apostrophes, and hyphens and throws out other punctuation and numbers (since this destroys ordering information, I put it into a new column called &lt;code&gt;tokens&lt;/code&gt;).&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-7&quot; title=&quot;7&quot;&gt;df[df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].&lt;span class=&quot;bu&quot;&gt;str&lt;/span&gt;.match(&lt;span class=&quot;st&quot;&gt;&amp;quot;[^&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\x00&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ch&quot;&gt;\x7F&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;]&amp;quot;&lt;/span&gt;)][&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-6&quot; title=&quot;6&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Check for remaining non-ASCII characters&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-5&quot; title=&quot;5&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-4&quot; title=&quot;4&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;].&lt;span class=&quot;bu&quot;&gt;apply&lt;/span&gt;(unidecode)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-3&quot; title=&quot;3&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Remove special characters&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-2&quot; title=&quot;2&quot;&gt;&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb3&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb3-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;im&quot;&gt;from&lt;/span&gt; unidecode &lt;span class=&quot;im&quot;&gt;import&lt;/span&gt; unidecode&lt;/a&gt;
&lt;p&gt;I then processed the text. &lt;!-- were listed in foreign languages,  --&gt; 251 menu items contained non-ASCII characters, of which 126 were in French,&lt;a href=&quot;#fn3&quot; class=&quot;footnote-ref&quot; id=&quot;fnref3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; 56 Chinese, 21 German, 20 Swedish, 11 Greek, 10 Hindi, 3 Hungarian, and one lonely item in Polish. The remaining three were English with special characters such as &#189;. Happily, the excellent Python library &lt;a href=&quot;https://pypi.org/project/Unidecode/&quot;&gt;Unidecode&lt;/a&gt; can do most of the heavy lifting here, stripping accents, and Romanizing the Greek, Hindi, and Chinese.&lt;/p&gt;
&lt;p&gt;Like the erroneous dates, these were sparse&#8212;only 6,682, or 0.5% of the listed dishes exceeded 100 characters, and these held only 3.7% of the dataset&#8217;s total characters. However, the decision to drop them was less clear-cut, since they potentially contained period-specific text which could be used to inform a model. I ultimately stetted them for this reason.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Afternoon Tea- A Great British Tradition- Tea, the most universally consumed of all drinks, is especially popular in Britain where the annual consumption is something in the region of 512 million cups. W. E. Gladstone observed &#8220;If you are cold, tea will warm you- if you are heated, it will cool you- if you are depressed, it will cheer you- if you are excited, it will calm you.&#8221; First brought to England c. 1559 by Giambattista Rusmusio, tea did not evolve into an afternoon meal until the end of the 18th century. Anna, Duchess of Bedford, invented afternoon tea to fill the long gap between early lunch and dinner which bored many house parties. It became a meal surrounded by etiquette and customs, delicate china, silver, cake stands and doilies- a time when friend and family meet. Famous tea parties include Mad Hatter&#8217;s (Alice&#8217;s Adventures in Wonderland by Lewis Carroll 1865), the Boston Tea Party, 1773, and not forgetting HM Queen Elizabeth II&#8217;s annual garden parties at Buckingham Palace. The Duke of Wellington declared that &#8220;Tea cleared my head and left no misapprehensions.&#8221; He was right- tea contains small amounts of two B vitamins, and has no calories, artificial flavourings or colourings. It is said to cure gout, apoplexy, epilepsy, gall stones and sleepiness, and one&#8217;s longevity is assured. &#8220;Thank God for Tea! What would the world do without tea?&#8221;- Sydney Smith&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As the histogram in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:length&quot;&gt;@fig:length&lt;/span&gt; shows, these go well beyond gusty descriptions like our &#8220;poached farmed Norwegian salmon&#8221;&#8212;the longest, from first class on a &lt;a href=&quot;http://menus.nypl.org/menus/29358&quot;&gt;1993 Virgin Atlantic flight&lt;/a&gt;, reads with the paragraph breaks removed like a deranged Basil Fawlty:&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-name-length.svg&quot; alt=&quot;Character length of menu items (log10 scale)&quot; id=&quot;fig:length&quot; /&gt;&lt;figcaption&gt;Character length of menu items (log&lt;sub&gt;10&lt;/sub&gt; scale)&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;The dish names themselves were somewhat less tractable. The menus are transcribed by hand, eliminating the need to deal with OCR errors, but many items were unreasonably long.&lt;/p&gt;
&lt;h2 id=&quot;dishes&quot;&gt;Dishes&lt;/h2&gt;
&lt;p&gt;That last menu, a &lt;a href=&quot;http://menus.nypl.org/menus/31054&quot;&gt;1933 Plaza Hotel menu&lt;/a&gt;, is actually 62 menus bound together, totalling 4,060 dishes. However, even the runner-up, a &lt;a href=&quot;http://menus.nypl.org/menus/34201&quot;&gt;1914 Waldorf Astoria menu&lt;/a&gt;, packs 1,360 dishes into ten octavo-sized pages. (For reference, the famously voluminous Cheesecake Factory menu has &lt;a href=&quot;https://www.mentalfloss.com/article/566482/why-the-cheesecake-factorys-menu-is-so-big&quot;&gt;&#8220;only&#8221;&lt;/a&gt; 250 items.) This shouldn&#8217;t matter to the model, of course, since it will be trained only on individual dish names.&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-item-length.svg&quot; alt=&quot;Number of items on menus (log10 scale)&quot; id=&quot;fig:items&quot; /&gt;&lt;figcaption&gt;Number of items on menus (log&lt;sub&gt;10&lt;/sub&gt; scale)&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;Looking at the number of items featured on each menu, we see that although 22.7% of them feature fewer than a hundred items, several reach into the thousands, with a startling outlier:&lt;/p&gt;
&lt;h2 id=&quot;menus&quot;&gt;Menus&lt;/h2&gt;
&lt;p&gt;This could cause problems during modeling, not only because a na&#239;ve model could latch onto the majority class and return it without considering the inputs, but because the meagerness of data&#8212;particularly before 1880 and after 1990&#8212;will leave the model with inadequate information to categorize &lt;em&gt;any&lt;/em&gt; menu as being from these eras.&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-items-by-decade.svg&quot; alt=&quot;Number of menus (left) and menu items (right) by decade&quot; id=&quot;fig:decade&quot; /&gt;&lt;figcaption&gt;Number of menus (left) and menu items (right) by decade&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;A further problem is that our dataset&#8217;s classes are heavily unbalanced. Looking at &lt;span class=&quot;citation&quot; data-cites=&quot;fig:decade&quot;&gt;@fig:decade&lt;/span&gt;, we see that the great preponderance of them&#8212;a full 63% of menus and 62% of items&#8212;are from the initial two decades of the twentieth century.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-6&quot; title=&quot;6&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;decade&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[&lt;span class=&quot;st&quot;&gt;&amp;quot;year&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;10&lt;/span&gt;&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-5&quot; title=&quot;5&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;year&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[&lt;span class=&quot;st&quot;&gt;&amp;quot;date&amp;quot;&lt;/span&gt;].dt.year&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-4&quot; title=&quot;4&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Calculate year and decade&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-3&quot; title=&quot;3&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-2&quot; title=&quot;2&quot;&gt;df[&lt;span class=&quot;st&quot;&gt;&amp;quot;date&amp;quot;&lt;/span&gt;] &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.to_datetime(df[&lt;span class=&quot;st&quot;&gt;&amp;quot;date&amp;quot;&lt;/span&gt;], errors&lt;span class=&quot;op&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;st&quot;&gt;&amp;quot;coerce&amp;quot;&lt;/span&gt;)&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb2&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb2-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Drop malformed dates&lt;/span&gt;&lt;/a&gt;
&lt;p&gt;The first work to be done was on the dates. For example, 638 dates turned out to be incorrect or malformed. In some, like &lt;code&gt;1091-01-27&lt;/code&gt;, the error was transparent, but with less than 0.05% of the data so corrupted, I decided to just drop them. However, another 68,438 items&#8212;5% of the data&#8212;were missing dates altogether. Since they were useless to the analysis, and I couldn&#8217;t know if there was a pattern to the missingness, I was forced to drop these as well. Then, from the 1.27 million well-formed dates remaining, I extracted the year and decade, the latter of which would prove a more reasonable target for modeling than the former.&lt;/p&gt;
&lt;h2 id=&quot;dates&quot;&gt;Dates&lt;/h2&gt;
&lt;h1 id=&quot;data-cleaning&quot;&gt;Data cleaning&lt;/h1&gt;
&lt;p&gt;This left me with a single data frame of menu items to clean and parse.&lt;/p&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-29&quot; title=&quot;29&quot;&gt;df &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df[[&lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;date&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;menu_id&amp;quot;&lt;/span&gt;]]&lt;/a&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-28&quot; title=&quot;28&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Remove intermediate columns&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-27&quot; title=&quot;27&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-26&quot; title=&quot;26&quot;&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-25&quot; title=&quot;25&quot;&gt;    right_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-24&quot; title=&quot;24&quot;&gt;    left_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;dish_id&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-23&quot; title=&quot;23&quot;&gt;    how &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;right&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-22&quot; title=&quot;22&quot;&gt;    right &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; dish[[&lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;]],&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-21&quot; title=&quot;21&quot;&gt;    left &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-20&quot; title=&quot;20&quot;&gt;df &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.merge(&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-19&quot; title=&quot;19&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Add dish name to each menu item&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-18&quot; title=&quot;18&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-17&quot; title=&quot;17&quot;&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-16&quot; title=&quot;16&quot;&gt;    right_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-15&quot; title=&quot;15&quot;&gt;    left_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;menu_id&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-14&quot; title=&quot;14&quot;&gt;    how &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;right&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-13&quot; title=&quot;13&quot;&gt;    right &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; menu[[&lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;date&amp;quot;&lt;/span&gt;]],&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-12&quot; title=&quot;12&quot;&gt;    left &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; df,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-11&quot; title=&quot;11&quot;&gt;df &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.merge(&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-10&quot; title=&quot;10&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Add menu date to each menu id&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-9&quot; title=&quot;9&quot;&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-8&quot; title=&quot;8&quot;&gt;)&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-7&quot; title=&quot;7&quot;&gt;    right_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-6&quot; title=&quot;6&quot;&gt;    left_on &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;menu_page_id&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-5&quot; title=&quot;5&quot;&gt;    how &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;quot;right&amp;quot;&lt;/span&gt;,&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-4&quot; title=&quot;4&quot;&gt;    right &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; menu_page[[&lt;span class=&quot;st&quot;&gt;&amp;quot;id&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;menu_id&amp;quot;&lt;/span&gt;]],&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-3&quot; title=&quot;3&quot;&gt;    left &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; menu_item[[&lt;span class=&quot;st&quot;&gt;&amp;quot;dish_id&amp;quot;&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;quot;menu_page_id&amp;quot;&lt;/span&gt;]],&lt;/a&gt;
&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-2&quot; title=&quot;2&quot;&gt;df &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; pd.merge(&lt;/a&gt;
&lt;div class=&quot;sourceCode&quot; id=&quot;cb1&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;a class=&quot;sourceLine&quot; id=&quot;cb1-1&quot; title=&quot;1&quot;&gt;&lt;span class=&quot;co&quot;&gt;# Add menu id to each menu item&lt;/span&gt;&lt;/a&gt;
&lt;p&gt;Since all I wanted was the name of the dish (from &lt;code&gt;Dish.csv&lt;/code&gt;), the date (from &lt;code&gt;Menu.csv&lt;/code&gt;), and the menu ID (in case I wanted to group dishes by menu), I merged the data frames like so:&lt;/p&gt;
&lt;/figure&gt;
&lt;img src=&quot;menu-schema.svg&quot; alt=&quot;Menu dataset table schema (some columns omitted). Source code&quot; id=&quot;fig:schema&quot; /&gt;&lt;figcaption&gt;Menu dataset table schema (some columns omitted). &lt;a href=&quot;https://gist.github.com/alexklapheke/a197eed2a9ba742aa0080c1bdbfa579c&quot;&gt;Source code&lt;/a&gt;&lt;/figcaption&gt;
&lt;figure&gt;
&lt;p&gt;The NYPL provides the data in the very simple &lt;a href=&quot;https://en.wikipedia.org/wiki/Snowflake_schema&quot;&gt;snowflake schema&lt;/a&gt; shown in &lt;span class=&quot;citation&quot; data-cites=&quot;fig:schema&quot;&gt;@fig:schema&lt;/span&gt;. The central table is &lt;code&gt;MenuItem.csv&lt;/code&gt;, in which each of the 1,334,417 rows represents an item on a menu. Each references a particular dish, which are named in &lt;code&gt;Dish.csv&lt;/code&gt; (426,959 comestibles in total), and each is also referenced to a page in &lt;code&gt;MenuPage.csv&lt;/code&gt;, which are in turn referenced in &lt;code&gt;Menu.csv&lt;/code&gt; to the particular bills of fare on which they appear.&lt;/p&gt;
&lt;h1 id=&quot;data-structuring&quot;&gt;Data structuring&lt;/h1&gt;
&lt;p&gt;By 1941, of course, the Oyster Bar&#8217;s menu lists &#8220;alligator pear salad&#8221; for 45&#162; and &#8220;lobster pan roast&#8221;&#8212;one of the priciest items named&#8212;for $1.45.&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; In this project I wanted to see if, using this data,&lt;a href=&quot;#fn2&quot; class=&quot;footnote-ref&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; I could predict the year a menu was served based on the dishes listed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Up until sometime in the 1800s, though, lobster was literally low-class food, eaten only by the poor and institutionalized. Even in the harsh penal environment of early America, some colonies had laws against feeding lobsters to inmates more than once a week because it was thought to be cruel and unusual, like making people eat rats. One reason for their low status was how plentiful lobsters were in old New England. &#8220;Unbelievable abundance&#8221; is how one source describes the situation, including accounts of Plymouth pilgrims wading out and capturing all they wanted by hand, and of early Boston&#8217;s seashore being littered with lobsters after hard storms&#8212;these latter were treated as a smelly nuisance and ground up for fertilizer. There is also the fact that premodern lobster was often cooked dead and then preserved, usually packed in salt or crude hermetic containers. Maine&#8217;s earliest lobster industry was based around a dozen such seaside canneries in the 1840s, from which lobster was shipped as far away as California, in demand only because it was cheap and high in protein, basically chewable fuel.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;These vicissitudes are due partly to economics. In 1914, avocados, then known as &#8220;alligator pears&#8221;, &lt;a href=&quot;https://time.com/4832655/avocado-american-history/&quot;&gt;could go for&lt;/a&gt; $1 each&#8212;more that $25 today. But economics can be adulterated by public perception. David Foster Wallace, considering the lobster, wrote that &lt;span class=&quot;citation&quot; data-cites=&quot;wallace2004consider&quot;&gt;[-@wallace2004consider,p. 55]&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;Since 2011, the New York Public Library has maintained &lt;a href=&quot;http://menus.nypl.org/&quot;&gt;&#8220;What&#8217;s on the menu?&#8221;&lt;/a&gt;, a collection of tens of thousands of restaurant menus going back to the mid-nineteenth century. This is an invaluable collection not just for data scientists, but for food historians, since it is well known that foods go in and out of fashion like clothing. For example, the famed &lt;a href=&quot;https://en.wikipedia.org/wiki/Grand_Central_Oyster_Bar_%26_Restaurant&quot;&gt;Oyster Bar&lt;/a&gt; at Grand Central Terminal, which in 1941 &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Grand_Central_Terminal_Restaurant_menu_1941.jpg&quot;&gt;featured&lt;/a&gt; &#8220;cream of chicken &#224; la reine&#8221;, &#8220;broiled sweetbreads on toast with Virginia ham&#8221;, and &#8220;farina custard pudding, Melba sauce&#8221; (not to mention oysters for a nickel apiece), today &lt;a href=&quot;https://web.archive.org/web/20160315171449/http://www.oysterbarny.com/pdf/dailymenu.pdf&quot;&gt;serves&lt;/a&gt; such mouthfuls as &#8220;poached farmed Norwegian salmon over baby red oak-watercress salad with charred scallion-honey vinaigrette, avocado, and goat cheese.&#8221;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For instance, the first phase is characterized by the question &#8220;How can we eat?&#8221;, the second by the question &#8220;Why do we eat?&#8221; and the third by the question, &#8220;Where shall we have lunch?&#8221;&lt;/p&gt;
&lt;p&gt;The History of every major Galactic Civilization tends to pass through three distinct and recognizable phases, those of Survival, Inquiry and Sophistication, otherwise known as the How, Why and Where phases.&lt;/p&gt;
&lt;div class=&quot;epigraph&quot;&gt;
	<description>
			<pubDate>2020-07-03T18:11:59-0400</pubDate>
			<link>https://alexklapheke.github.io/blog/publish/menu.md</link>
			<title>Menu Categorization</title>
		<item>
